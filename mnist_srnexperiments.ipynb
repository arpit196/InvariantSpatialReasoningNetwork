{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "0VDarlagmZpQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10,cifar100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vURBdS9QmZpR"
      },
      "outputs": [],
      "source": [
        "from . SpatialReasoningModel import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBD15N5JmZpR"
      },
      "outputs": [],
      "source": [
        "# example of loading the mnist dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn import datasets\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#from tensorflow.keras.datasets import cifar10,cifar100\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "def conc(*inp1):\n",
        "  return layers.Concatenate()(inp1)\n",
        "\n",
        "#!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "def mpool(psize,strides=2):\n",
        "  return MaxPooling2D(pool_size=psize,strides=strides,padding=\"SAME\")\n",
        "\n",
        "def apool(psize,strides=None):\n",
        "  if(strides is None):\n",
        "    return AveragePooling2D(pool_size=psize,padding=\"SAME\")\n",
        "  else:\n",
        "    return AveragePooling2D(pool_size=psize,strides=strides,padding=\"SAME\")\n",
        "\n",
        "def ln():\n",
        "  return layers.LayerNormalization()\n",
        "\n",
        "def bn():\n",
        "  return layers.BatchNormalization()\n",
        "\n",
        "def dense(size,act='relu'):\n",
        "  return Dense(size,activation=act)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow.keras.layers as layers\n",
        "filepath = 'my_best_model.hdf5'\n",
        "\n",
        "def bnconv(inp,units,kernel_size):\n",
        "  return BatchNormalization()(Conv2D(units,kernel_size,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005))(inp))\n",
        "\n",
        "def dense(units,act='relu'):\n",
        "  return layers.Dense(units,activation=act)\n",
        "\n",
        "def edim(inp,axis=-1):\n",
        "    return tf.expand_dims(inp,axis)\n",
        "\n",
        "def conv(inp,units,kernel_size):\n",
        "  return Conv2D(kernel_size,units,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005))(inp)\n",
        "\n",
        "def cutout(image,size):\n",
        "  return tfa.image.cutout(image,mask_size=(size,size))\n",
        "\n",
        "def LaplacianFilter(input_tensor):\n",
        "  filter = tf.constant([[[0.0,-1.0,0.0],[-1.0,4.0,-1.0],[0.0,-1.0,0.0]]])   #The filters for derivatives Cx, Cy ,[[0.0,-1.0],[1.0,0.0]]\n",
        "  filter = filter[:,:,:,tf.newaxis]\n",
        "  filter = tf.transpose(filter,[1,2,0,3])\n",
        "  print(filter.shape)\n",
        "  tf.print(filter[:,:,1,1])\n",
        "  out = tf.nn.conv2d(input_tensor,filters=filter,padding=\"SAME\",strides=[1, 1, 1, 1])  #basic convolution operation in tensorflow, the derivative filters are applied with stride 3\n",
        "  return out\n",
        "\n",
        "\n",
        "def LocalCurvature(input_tensor,scale=1,edge=True):\n",
        "  filter = tf.constant([[[-1.0,0.0],[0.0,0.0]],[[1.0,0.0],[1.0,-1.0]]])   #The filters for derivatives Cx, Cy ,[[0.0,-1.0],[1.0,0.0]]\n",
        "  filter = tf.transpose(filter,[1,2,0])\n",
        "  filter = filter[:,:,tf.newaxis] #Expanding according to number of input channels\n",
        "  filter = tf.tile(filter,[1,1,input_tensor.shape[-1],1])\n",
        "  filter2 = tf.constant([[[0.0,0.0],[0.0,0.0]],[[1.0,0.0],[0.0,1.0]]])   #The filters for derivatives Cx, Cy ,[[0.0,1.0],[0.0,0.0]]\n",
        "  filter2 = tf.transpose(filter2,[1,2,0])\n",
        "\n",
        "  filter2 = filter2[:,:,tf.newaxis]\n",
        "  filter2 = tf.tile(filter2,[1,1,input_tensor.shape[-1],1])                                         #Expanding according to number of input channels\n",
        "  filter3 = tf.constant([[[1.0,0.0],[0.0,0.0]],[[0.0,0.0],[1.0,0.0]]])   #The filters for derivatives Cx, Cy ,[[0.0,0.0],[1.0,0.0]\n",
        "  filter3 = tf.transpose(filter3,[1,2,0])\n",
        "  filter3 = filter3[:,:,tf.newaxis]\n",
        "  filter3 = tf.tile(filter3,[1,1,input_tensor.shape[-1],1])\n",
        "  out2 = tf.nn.conv2d(input_tensor, filters=filter2,padding=\"SAME\",dilations=scale,strides=[1, 1, 1, 1])\n",
        "  out3 = tf.nn.conv2d(input_tensor, filters=filter3,padding=\"SAME\",dilations=scale,strides=[1, 1, 1, 1])\n",
        "  out = tf.math.abs(tf.nn.conv2d(input_tensor,filters=filter,dilations=scale,padding=\"SAME\",strides=[1, 1, 1, 1]))  #basic convolution operation in tensorflow, the derivative filters are applied with stride 3\n",
        "  if edge:\n",
        "    return tf.where(tf.abs(out2*out3)>0,out,0.0)\n",
        "  else:\n",
        "    return out\n",
        "\n",
        "def LocalCurvature(input_tensor,scale=1,edge=True):\n",
        "  filter = tf.constant([[[-1.0,0.0],[1.0,0.0]],[[0.0,0.0],[1.0,-1.0]],[[0.0,-1.0],[1.0,0.0]]])   #The filters for derivatives Cx, Cy\n",
        "  filter = filter[:,:,tf.newaxis]                                         #Expanding according to number of input channels\n",
        "  filter = tf.tile(filter,[1,1,1,1])\n",
        "  filter2 = tf.constant([[[0.0,0.0],[1.0,0.0]],[[0.0,0.0],[0.0,1.0]],[[0.0,1.0],[0.0,0.0]]])   #The filters for derivatives Cx, Cy\n",
        "  filter2 = filter2[:,:,tf.newaxis]                                         #Expanding according to number of input channels\n",
        "  filter3 = tf.constant([[[1.0,0.0],[0.0,0.0]],[[0.0,0.0],[1.0,0.0]],[[0.0,0.0],[1.0,0.0]]])   #The filters for derivatives Cx, Cy\n",
        "  filter3 = filter3[:,:,tf.newaxis]\n",
        "  filter2 = tf.tile(filter2,[1,1,1,1])\n",
        "  out2 = tf.nn.conv2d(input_tensor, filters=filter2,padding=\"SAME\",dilations=scale,strides=[1, 1, 1, 1])\n",
        "  out3 = tf.nn.conv2d(input_tensor, filters=filter3,padding=\"SAME\",dilations=scale,strides=[1, 1, 1, 1])\n",
        "  out = tf.math.abs(tf.nn.conv2d(input_tensor,filters=filter,dilations=scale,padding=\"SAME\",strides=[1, 1, 1, 1]))  #basic convolution operation in tensorflow, the derivative filters are applied with stride 3\n",
        "  if edge:\n",
        "    return tf.where(tf.abs(out2*out3)>0,out,0.0)\n",
        "  else:\n",
        "    return out\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_imagecont(image, label,image_size=28):\n",
        "  image = tf.convert_to_tensor(image)\n",
        "  image = tf.image.resize(image, [image_size,image_size])\n",
        "  image1 = tfa.image.gaussian_filter2d(image, (2,2),3)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  #image = image/255.0\n",
        "  positionsx1 = tf.range(start=0, limit=image_size, delta=float(1),dtype=tf.float32)\n",
        "  positionsy1 = tf.range(start=0, limit=image_size, delta=float(1),dtype=tf.float32)\n",
        "  positionsx1 = tf.expand_dims(tf.tile(tf.expand_dims(positionsx1,0),[image_size,1]),-1)\n",
        "  positionsy1 = tf.expand_dims(tf.tile(tf.transpose(tf.expand_dims(positionsy1,0)),[1,image_size]),-1)\n",
        "  positions11 = tf.concat([positionsx1,positionsy1],-1); positions11 = tf.tile(positions11[tf.newaxis,:,:,:],[image.shape[0],1,1,1])\n",
        "  u = tf.image.sobel_edges(image1)\n",
        "  angle = tf.where(u[:,:,:,0,0]!=0,tf.atan2(u[:,:,:,0,1],u[:,:,:,0,0]),0)\n",
        "  angle = angle[:,:,:,tf.newaxis]\n",
        "  #u = tf.reshape(u,[-1,image_size,image_size,6])\n",
        "\n",
        "  image = tf.concat([image,angle/3.14,positions11],-1)\n",
        "  #image = tf.concat([u[tf.newaxis,:,:,:],angle,positions11[tf.newaxis,:]],-1)\n",
        "  #image=tf.squeeze(image,0)\n",
        "  return image, label\n",
        "\n",
        "labeled_batch_size=75\n",
        "num_epochs = 60\n",
        "batch_size = 30  # Corresponds to 200 steps per epoch\n",
        "width = 28\n",
        "temperature = 1\n",
        "learning_rate=0.0001\n",
        "#lr_drop=20\n",
        "\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "#select = tf.convert_to_tensor(select,dtype=tf.float32)\n",
        "\n",
        "def train(model,path,epochs=100):\n",
        "    checkpoint1 = ModelCheckpoint(filepath='./'+path,save_format=tf,monitor='val_loss',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(0.1,momentum=0.9),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics='categorical_accuracy')\n",
        "    model.fit(trainXA, trainY, epochs=epochs, batch_size=200, validation_data=(testXA, testY), callbacks=[checkpoint1, reduce_lr], verbose=1)\n",
        "\n",
        "learning_rate = 0.1\n",
        "lr_drop=15\n",
        "def lr_scheduler(epoch):\n",
        "        return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "\n",
        "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "checkpoint_gen = ModelCheckpoint(filepath='./'+'generative_cifar_2',save_format=tf,monitor='val_val_loss',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "image_size=28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk73cJFamZpS"
      },
      "outputs": [],
      "source": [
        "image_size=28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6D2louamZpS"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "\t# load dataset\n",
        "  (trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "  trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "  testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "  return trainX, trainY, testX, testY\n",
        "\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "  train_norm = train.astype('float32')\n",
        "  test_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "  train_norm = train_norm / 255.0\n",
        "  test_norm = test_norm / 255.0\n",
        "  normalization = layers.Normalization()\n",
        "  normalization.adapt(train_norm)\n",
        "  #train_norm=normalization(train_norm)\n",
        "  #test_norm=normalization(test_norm)\n",
        "  return train_norm, test_norm\n",
        "\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "\t# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)\n",
        "trainXA,trainY = preprocess_imagecont(trainX,trainY)\n",
        "testXA,testY = preprocess_imagecont(testX,testY)\n",
        "#train_images_res = tf.image.resize(trainXA,[48,48],method='bilinear')\n",
        "#test_images_res = tf.image.resize(testXA,[48,48],method='bilinear')\n",
        "\n",
        "trainY = tf.keras.utils.to_categorical(trainY,num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(testY,num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxCRF7ANmZpS",
        "outputId": "6f12b5fa-e07c-4e5a-c12b-53b26afb3d9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([10000, 28, 28, 4])"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testXA.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyUqkeAMmZpT"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wDArQKJmZpT"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001\n",
        "lr_drop=20\n",
        "def lr_scheduler(epoch):\n",
        "        return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "\n",
        "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "checkpoint_gen = ModelCheckpoint(filepath='./'+'generative_cifar_2',save_format=tf,monitor='val_val_loss',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV1LgFG_mZpT"
      },
      "outputs": [],
      "source": [
        "def train(model,path,epochs=80):\n",
        "  checkpoint1 = ModelCheckpoint(filepath='./'+path,save_format=tf,monitor='val_categorical_accuracy',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='max')\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics='categorical_accuracy')\n",
        "  model.fit(trainXA,trainY, epochs=epochs, batch_size=30, validation_data=(testXA,testY), callbacks=[reduce_lr,checkpoint1], verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtibYf8DnJvL"
      },
      "source": [
        "**Load CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDpzndhgmZpT",
        "outputId": "64ec17cc-bf47-4f9f-c124-9cac82701276"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1fc99e22070>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c = ConvolutionalModel()\n",
        "c.load_weights('conv_4_mnist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O-SebganC-5"
      },
      "source": [
        "**Invariance of CNN model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIhPR87jmZpU",
        "outputId": "a7db2048-0212-46fb-b375-c86e92555154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original accuracy\n",
            "99.45000410079956%\n",
            "Accuracy on Images scaled by factor:\n",
            "1.1\n",
            "97.54000306129456%\n",
            "1.2\n",
            "81.6800057888031%\n",
            "1.3\n",
            "55.250000953674316%\n",
            "1.4\n",
            "45.00000178813934%\n",
            "Accuracy on Images sheared by factor:\n",
            "0.1\n",
            "97.76000380516052%\n",
            "0.2\n",
            "87.55000233650208%\n",
            "0.4\n",
            "27.100002765655518%\n",
            "0.5\n",
            "13.220000267028809%\n"
          ]
        }
      ],
      "source": [
        "test_invariance(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-04T13:23:42.802818Z",
          "iopub.status.busy": "2024-02-04T13:23:42.802439Z",
          "iopub.status.idle": "2024-02-04T14:23:43.662753Z",
          "shell.execute_reply": "2024-02-04T14:23:43.660923Z",
          "shell.execute_reply.started": "2024-02-04T13:23:42.802786Z"
        },
        "id": "Q2XdLuPDmZpU",
        "outputId": "60d66672-1b74-4c6a-e347-62044637014b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 7, 7, 840)\n",
            "inputsshape(None, 7, 7, 13440)\n",
            "(None, 1240)\n",
            "Epoch 1/80\n",
            "(30, 7, 7, 840)\n",
            "inputsshape(30, 7, 7, 13440)\n",
            "(30, 1240)\n",
            "(30, 7, 7, 840)\n",
            "inputsshape(30, 7, 7, 13440)\n",
            "(30, 1240)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-04 13:23:45.657333: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/tf.where_4/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1998/2000 [============================>.] - ETA: 0s - loss: 1.2575 - categorical_accuracy: 0.9335(None, 7, 7, 840)\n",
            "inputsshape(None, 7, 7, 13440)\n",
            "(None, 1240)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-04 13:24:36.997778: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/tf.where_4/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.96500, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 57s 26ms/step - loss: 1.2572 - categorical_accuracy: 0.9335 - val_loss: 1.0078 - val_categorical_accuracy: 0.9650 - lr: 1.0000e-04\n",
            "Epoch 2/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.8909 - categorical_accuracy: 0.9711\n",
            "Epoch 2: val_categorical_accuracy improved from 0.96500 to 0.97110, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 26ms/step - loss: 0.8909 - categorical_accuracy: 0.9710 - val_loss: 0.7950 - val_categorical_accuracy: 0.9711 - lr: 1.0000e-04\n",
            "Epoch 3/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.6857 - categorical_accuracy: 0.9773\n",
            "Epoch 3: val_categorical_accuracy improved from 0.97110 to 0.98020, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 26ms/step - loss: 0.6858 - categorical_accuracy: 0.9773 - val_loss: 0.5925 - val_categorical_accuracy: 0.9802 - lr: 1.0000e-04\n",
            "Epoch 4/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.5167 - categorical_accuracy: 0.9813\n",
            "Epoch 4: val_categorical_accuracy improved from 0.98020 to 0.98060, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.5167 - categorical_accuracy: 0.9813 - val_loss: 0.4539 - val_categorical_accuracy: 0.9806 - lr: 1.0000e-04\n",
            "Epoch 5/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.3845 - categorical_accuracy: 0.9853\n",
            "Epoch 5: val_categorical_accuracy did not improve from 0.98060\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.3844 - categorical_accuracy: 0.9853 - val_loss: 0.3771 - val_categorical_accuracy: 0.9765 - lr: 1.0000e-04\n",
            "Epoch 6/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.2999 - categorical_accuracy: 0.9862\n",
            "Epoch 6: val_categorical_accuracy improved from 0.98060 to 0.98460, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.2999 - categorical_accuracy: 0.9862 - val_loss: 0.2811 - val_categorical_accuracy: 0.9846 - lr: 1.0000e-04\n",
            "Epoch 7/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.2417 - categorical_accuracy: 0.9878\n",
            "Epoch 7: val_categorical_accuracy improved from 0.98460 to 0.98490, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.2418 - categorical_accuracy: 0.9878 - val_loss: 0.2348 - val_categorical_accuracy: 0.9849 - lr: 1.0000e-04\n",
            "Epoch 8/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1979 - categorical_accuracy: 0.9901\n",
            "Epoch 8: val_categorical_accuracy did not improve from 0.98490\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1979 - categorical_accuracy: 0.9901 - val_loss: 0.2532 - val_categorical_accuracy: 0.9694 - lr: 1.0000e-04\n",
            "Epoch 9/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.1684 - categorical_accuracy: 0.9912\n",
            "Epoch 9: val_categorical_accuracy improved from 0.98490 to 0.98820, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1683 - categorical_accuracy: 0.9912 - val_loss: 0.1672 - val_categorical_accuracy: 0.9882 - lr: 1.0000e-04\n",
            "Epoch 10/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1435 - categorical_accuracy: 0.9922\n",
            "Epoch 10: val_categorical_accuracy did not improve from 0.98820\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.1435 - categorical_accuracy: 0.9923 - val_loss: 0.1670 - val_categorical_accuracy: 0.9830 - lr: 1.0000e-04\n",
            "Epoch 11/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1263 - categorical_accuracy: 0.9929\n",
            "Epoch 11: val_categorical_accuracy improved from 0.98820 to 0.98850, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1263 - categorical_accuracy: 0.9929 - val_loss: 0.1378 - val_categorical_accuracy: 0.9885 - lr: 1.0000e-04\n",
            "Epoch 12/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.1104 - categorical_accuracy: 0.9943\n",
            "Epoch 12: val_categorical_accuracy did not improve from 0.98850\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1104 - categorical_accuracy: 0.9943 - val_loss: 0.1246 - val_categorical_accuracy: 0.9879 - lr: 1.0000e-04\n",
            "Epoch 13/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1005 - categorical_accuracy: 0.9942\n",
            "Epoch 13: val_categorical_accuracy did not improve from 0.98850\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1005 - categorical_accuracy: 0.9942 - val_loss: 0.1247 - val_categorical_accuracy: 0.9856 - lr: 1.0000e-04\n",
            "Epoch 14/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0907 - categorical_accuracy: 0.9947\n",
            "Epoch 14: val_categorical_accuracy did not improve from 0.98850\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0907 - categorical_accuracy: 0.9947 - val_loss: 0.1149 - val_categorical_accuracy: 0.9867 - lr: 1.0000e-04\n",
            "Epoch 15/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0836 - categorical_accuracy: 0.9953\n",
            "Epoch 15: val_categorical_accuracy did not improve from 0.98850\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0836 - categorical_accuracy: 0.9953 - val_loss: 0.1388 - val_categorical_accuracy: 0.9790 - lr: 1.0000e-04\n",
            "Epoch 16/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0684 - categorical_accuracy: 0.9983\n",
            "Epoch 16: val_categorical_accuracy improved from 0.98850 to 0.99190, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 26ms/step - loss: 0.0684 - categorical_accuracy: 0.9983 - val_loss: 0.0869 - val_categorical_accuracy: 0.9919 - lr: 5.0000e-05\n",
            "Epoch 17/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0605 - categorical_accuracy: 0.9990\n",
            "Epoch 17: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0605 - categorical_accuracy: 0.9990 - val_loss: 0.0854 - val_categorical_accuracy: 0.9904 - lr: 5.0000e-05\n",
            "Epoch 18/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0563 - categorical_accuracy: 0.9986\n",
            "Epoch 18: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0563 - categorical_accuracy: 0.9986 - val_loss: 0.0802 - val_categorical_accuracy: 0.9901 - lr: 5.0000e-05\n",
            "Epoch 19/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0513 - categorical_accuracy: 0.9990\n",
            "Epoch 19: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0513 - categorical_accuracy: 0.9990 - val_loss: 0.0781 - val_categorical_accuracy: 0.9906 - lr: 5.0000e-05\n",
            "Epoch 20/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0477 - categorical_accuracy: 0.9988\n",
            "Epoch 20: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0477 - categorical_accuracy: 0.9988 - val_loss: 0.0755 - val_categorical_accuracy: 0.9902 - lr: 5.0000e-05\n",
            "Epoch 21/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0451 - categorical_accuracy: 0.9987\n",
            "Epoch 21: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0451 - categorical_accuracy: 0.9987 - val_loss: 0.0709 - val_categorical_accuracy: 0.9906 - lr: 5.0000e-05\n",
            "Epoch 22/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0426 - categorical_accuracy: 0.9988\n",
            "Epoch 22: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0426 - categorical_accuracy: 0.9988 - val_loss: 0.0757 - val_categorical_accuracy: 0.9894 - lr: 5.0000e-05\n",
            "Epoch 23/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0408 - categorical_accuracy: 0.9988\n",
            "Epoch 23: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0408 - categorical_accuracy: 0.9988 - val_loss: 0.0687 - val_categorical_accuracy: 0.9908 - lr: 5.0000e-05\n",
            "Epoch 24/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0392 - categorical_accuracy: 0.9987\n",
            "Epoch 24: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0392 - categorical_accuracy: 0.9987 - val_loss: 0.0661 - val_categorical_accuracy: 0.9911 - lr: 5.0000e-05\n",
            "Epoch 25/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0380 - categorical_accuracy: 0.9985\n",
            "Epoch 25: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0380 - categorical_accuracy: 0.9985 - val_loss: 0.0740 - val_categorical_accuracy: 0.9883 - lr: 5.0000e-05\n",
            "Epoch 26/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0359 - categorical_accuracy: 0.9989\n",
            "Epoch 26: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0359 - categorical_accuracy: 0.9989 - val_loss: 0.0748 - val_categorical_accuracy: 0.9877 - lr: 5.0000e-05\n",
            "Epoch 27/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0338 - categorical_accuracy: 0.9991\n",
            "Epoch 27: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0338 - categorical_accuracy: 0.9991 - val_loss: 0.0615 - val_categorical_accuracy: 0.9902 - lr: 5.0000e-05\n",
            "Epoch 28/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0343 - categorical_accuracy: 0.9987\n",
            "Epoch 28: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0343 - categorical_accuracy: 0.9987 - val_loss: 0.0620 - val_categorical_accuracy: 0.9903 - lr: 5.0000e-05\n",
            "Epoch 29/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0311 - categorical_accuracy: 0.9994\n",
            "Epoch 29: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0311 - categorical_accuracy: 0.9994 - val_loss: 0.0655 - val_categorical_accuracy: 0.9894 - lr: 5.0000e-05\n",
            "Epoch 30/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0318 - categorical_accuracy: 0.9989\n",
            "Epoch 30: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0318 - categorical_accuracy: 0.9989 - val_loss: 0.0573 - val_categorical_accuracy: 0.9905 - lr: 5.0000e-05\n",
            "Epoch 31/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0274 - categorical_accuracy: 0.9998\n",
            "Epoch 31: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0274 - categorical_accuracy: 0.9998 - val_loss: 0.0528 - val_categorical_accuracy: 0.9917 - lr: 2.5000e-05\n",
            "Epoch 32/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0255 - categorical_accuracy: 0.9998\n",
            "Epoch 32: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0255 - categorical_accuracy: 0.9998 - val_loss: 0.0600 - val_categorical_accuracy: 0.9889 - lr: 2.5000e-05\n",
            "Epoch 33/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0237 - categorical_accuracy: 0.9999\n",
            "Epoch 33: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0237 - categorical_accuracy: 0.9999 - val_loss: 0.0481 - val_categorical_accuracy: 0.9919 - lr: 2.5000e-05\n",
            "Epoch 34/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0226 - categorical_accuracy: 0.9997\n",
            "Epoch 34: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0226 - categorical_accuracy: 0.9997 - val_loss: 0.0471 - val_categorical_accuracy: 0.9910 - lr: 2.5000e-05\n",
            "Epoch 35/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0213 - categorical_accuracy: 0.9999\n",
            "Epoch 35: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0213 - categorical_accuracy: 0.9999 - val_loss: 0.0539 - val_categorical_accuracy: 0.9891 - lr: 2.5000e-05\n",
            "Epoch 36/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0203 - categorical_accuracy: 0.9999\n",
            "Epoch 36: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0203 - categorical_accuracy: 0.9999 - val_loss: 0.0476 - val_categorical_accuracy: 0.9906 - lr: 2.5000e-05\n",
            "Epoch 37/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0192 - categorical_accuracy: 0.9998\n",
            "Epoch 37: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0192 - categorical_accuracy: 0.9998 - val_loss: 0.0498 - val_categorical_accuracy: 0.9901 - lr: 2.5000e-05\n",
            "Epoch 38/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0183 - categorical_accuracy: 0.9999\n",
            "Epoch 38: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0183 - categorical_accuracy: 0.9999 - val_loss: 0.0455 - val_categorical_accuracy: 0.9910 - lr: 2.5000e-05\n",
            "Epoch 39/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0181 - categorical_accuracy: 0.9997\n",
            "Epoch 39: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0181 - categorical_accuracy: 0.9997 - val_loss: 0.0507 - val_categorical_accuracy: 0.9901 - lr: 2.5000e-05\n",
            "Epoch 40/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0176 - categorical_accuracy: 0.9997\n",
            "Epoch 40: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0176 - categorical_accuracy: 0.9997 - val_loss: 0.0473 - val_categorical_accuracy: 0.9914 - lr: 2.5000e-05\n",
            "Epoch 41/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0164 - categorical_accuracy: 0.9999\n",
            "Epoch 41: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0164 - categorical_accuracy: 0.9999 - val_loss: 0.0522 - val_categorical_accuracy: 0.9891 - lr: 2.5000e-05\n",
            "Epoch 42/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0164 - categorical_accuracy: 0.9998\n",
            "Epoch 42: val_categorical_accuracy improved from 0.99190 to 0.99220, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0164 - categorical_accuracy: 0.9998 - val_loss: 0.0444 - val_categorical_accuracy: 0.9922 - lr: 2.5000e-05\n",
            "Epoch 43/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0163 - categorical_accuracy: 0.9996\n",
            "Epoch 43: val_categorical_accuracy did not improve from 0.99220\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0163 - categorical_accuracy: 0.9997 - val_loss: 0.0428 - val_categorical_accuracy: 0.9919 - lr: 2.5000e-05\n",
            "Epoch 44/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0159 - categorical_accuracy: 0.9997\n",
            "Epoch 44: val_categorical_accuracy improved from 0.99220 to 0.99230, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0159 - categorical_accuracy: 0.9997 - val_loss: 0.0415 - val_categorical_accuracy: 0.9923 - lr: 2.5000e-05\n",
            "Epoch 45/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0155 - categorical_accuracy: 0.9998\n",
            "Epoch 45: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0155 - categorical_accuracy: 0.9998 - val_loss: 0.0496 - val_categorical_accuracy: 0.9900 - lr: 2.5000e-05\n",
            "Epoch 46/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0144 - categorical_accuracy: 1.0000\n",
            "Epoch 46: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0144 - categorical_accuracy: 1.0000 - val_loss: 0.0406 - val_categorical_accuracy: 0.9917 - lr: 1.2500e-05\n",
            "Epoch 47/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0136 - categorical_accuracy: 1.0000\n",
            "Epoch 47: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0136 - categorical_accuracy: 1.0000 - val_loss: 0.0419 - val_categorical_accuracy: 0.9915 - lr: 1.2500e-05\n",
            "Epoch 48/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0131 - categorical_accuracy: 1.0000\n",
            "Epoch 48: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0131 - categorical_accuracy: 1.0000 - val_loss: 0.0376 - val_categorical_accuracy: 0.9917 - lr: 1.2500e-05\n",
            "Epoch 49/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0123 - categorical_accuracy: 1.0000\n",
            "Epoch 49: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0123 - categorical_accuracy: 1.0000 - val_loss: 0.0386 - val_categorical_accuracy: 0.9916 - lr: 1.2500e-05\n",
            "Epoch 50/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0117 - categorical_accuracy: 1.0000\n",
            "Epoch 50: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0117 - categorical_accuracy: 1.0000 - val_loss: 0.0408 - val_categorical_accuracy: 0.9904 - lr: 1.2500e-05\n",
            "Epoch 51/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0112 - categorical_accuracy: 1.0000\n",
            "Epoch 51: val_categorical_accuracy improved from 0.99230 to 0.99250, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0112 - categorical_accuracy: 1.0000 - val_loss: 0.0346 - val_categorical_accuracy: 0.9925 - lr: 1.2500e-05\n",
            "Epoch 52/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0107 - categorical_accuracy: 1.0000\n",
            "Epoch 52: val_categorical_accuracy did not improve from 0.99250\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0107 - categorical_accuracy: 1.0000 - val_loss: 0.0369 - val_categorical_accuracy: 0.9911 - lr: 1.2500e-05\n",
            "Epoch 53/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0103 - categorical_accuracy: 1.0000\n",
            "Epoch 53: val_categorical_accuracy did not improve from 0.99250\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0103 - categorical_accuracy: 1.0000 - val_loss: 0.0343 - val_categorical_accuracy: 0.9922 - lr: 1.2500e-05\n",
            "Epoch 54/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0101 - categorical_accuracy: 1.0000\n",
            "Epoch 54: val_categorical_accuracy improved from 0.99250 to 0.99260, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0101 - categorical_accuracy: 1.0000 - val_loss: 0.0337 - val_categorical_accuracy: 0.9926 - lr: 1.2500e-05\n",
            "Epoch 55/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0096 - categorical_accuracy: 1.0000\n",
            "Epoch 55: val_categorical_accuracy improved from 0.99260 to 0.99290, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0096 - categorical_accuracy: 1.0000 - val_loss: 0.0345 - val_categorical_accuracy: 0.9929 - lr: 1.2500e-05\n",
            "Epoch 56/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0095 - categorical_accuracy: 0.9999\n",
            "Epoch 56: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0095 - categorical_accuracy: 0.9999 - val_loss: 0.0380 - val_categorical_accuracy: 0.9911 - lr: 1.2500e-05\n",
            "Epoch 57/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0092 - categorical_accuracy: 1.0000\n",
            "Epoch 57: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0092 - categorical_accuracy: 1.0000 - val_loss: 0.0352 - val_categorical_accuracy: 0.9920 - lr: 1.2500e-05\n",
            "Epoch 58/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0087 - categorical_accuracy: 1.0000\n",
            "Epoch 58: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0087 - categorical_accuracy: 1.0000 - val_loss: 0.0341 - val_categorical_accuracy: 0.9921 - lr: 1.2500e-05\n",
            "Epoch 59/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0087 - categorical_accuracy: 1.0000\n",
            "Epoch 59: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0087 - categorical_accuracy: 1.0000 - val_loss: 0.0385 - val_categorical_accuracy: 0.9911 - lr: 1.2500e-05\n",
            "Epoch 60/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0086 - categorical_accuracy: 1.0000\n",
            "Epoch 60: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0086 - categorical_accuracy: 1.0000 - val_loss: 0.0332 - val_categorical_accuracy: 0.9922 - lr: 1.2500e-05\n",
            "Epoch 61/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0081 - categorical_accuracy: 1.0000\n",
            "Epoch 61: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0081 - categorical_accuracy: 1.0000 - val_loss: 0.0344 - val_categorical_accuracy: 0.9915 - lr: 6.2500e-06\n",
            "Epoch 62/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0079 - categorical_accuracy: 1.0000\n",
            "Epoch 62: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 52s 26ms/step - loss: 0.0079 - categorical_accuracy: 1.0000 - val_loss: 0.0368 - val_categorical_accuracy: 0.9912 - lr: 6.2500e-06\n",
            "Epoch 63/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0077 - categorical_accuracy: 1.0000\n",
            "Epoch 63: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0077 - categorical_accuracy: 1.0000 - val_loss: 0.0333 - val_categorical_accuracy: 0.9920 - lr: 6.2500e-06\n",
            "Epoch 64/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0073 - categorical_accuracy: 1.0000\n",
            "Epoch 64: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0073 - categorical_accuracy: 1.0000 - val_loss: 0.0322 - val_categorical_accuracy: 0.9919 - lr: 6.2500e-06\n",
            "Epoch 65/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0071 - categorical_accuracy: 1.0000\n",
            "Epoch 65: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0071 - categorical_accuracy: 1.0000 - val_loss: 0.0312 - val_categorical_accuracy: 0.9926 - lr: 6.2500e-06\n",
            "Epoch 66/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0068 - categorical_accuracy: 1.0000\n",
            "Epoch 66: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0068 - categorical_accuracy: 1.0000 - val_loss: 0.0314 - val_categorical_accuracy: 0.9926 - lr: 6.2500e-06\n",
            "Epoch 67/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0066 - categorical_accuracy: 1.0000\n",
            "Epoch 67: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0066 - categorical_accuracy: 1.0000 - val_loss: 0.0341 - val_categorical_accuracy: 0.9918 - lr: 6.2500e-06\n",
            "Epoch 68/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0064 - categorical_accuracy: 1.0000\n",
            "Epoch 68: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0064 - categorical_accuracy: 1.0000 - val_loss: 0.0309 - val_categorical_accuracy: 0.9927 - lr: 6.2500e-06\n",
            "Epoch 69/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0062 - categorical_accuracy: 1.0000\n",
            "Epoch 69: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0062 - categorical_accuracy: 1.0000 - val_loss: 0.0330 - val_categorical_accuracy: 0.9916 - lr: 6.2500e-06\n",
            "Epoch 70/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0060 - categorical_accuracy: 1.0000\n",
            "Epoch 70: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0060 - categorical_accuracy: 1.0000 - val_loss: 0.0364 - val_categorical_accuracy: 0.9910 - lr: 6.2500e-06\n",
            "Epoch 71/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0058 - categorical_accuracy: 1.0000"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m62,_ \u001b[38;5;241m=\u001b[39m AffineModel()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm62\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2_lyr_srn_1conv_bn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[21], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, path, epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m checkpoint1 \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpath,save_format\u001b[38;5;241m=\u001b[39mtf,monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                           save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m                            verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      5\u001b[0m                            save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                            mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[1;32m      8\u001b[0m   loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m   metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1777\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1778\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[1;32m   1790\u001b[0m     )\n\u001b[0;32m-> 1791\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1804\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1806\u001b[0m }\n\u001b[1;32m   1807\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2198\u001b[0m             ):\n\u001b[1;32m   2199\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2200\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4000\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   4002\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "srn = SpatialRelationsModel()()\n",
        "train(srn,'2_lyr_srn_1conv_bn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnKI5BSgmZpU",
        "outputId": "750a2092-c08b-449f-fa8a-b40f282587d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 13s 32ms/step - loss: 0.0554 - categorical_accuracy: 0.9907\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.05539818853139877, 0.9907000660896301]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m64.evaluate(testXA,testY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i5h7esAnZ_S"
      },
      "source": [
        "***Testing Invariance of Single layer SRN with 2 convolutional layers***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWGzPkc_mZpU",
        "outputId": "16997df7-c330-4471-92b2-96ea41763bec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2377b113e20>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "srn.load_weights('single_layer_2conv_srn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htRlnehJmZpU",
        "outputId": "577e8c1d-72ad-4f29-f9c8-724ba89988ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original accuracy\n",
            "97.8600025177002%\n",
            "Accuracy on Images scaled by factor:\n",
            "1.1\n",
            "97.53000736236572%\n",
            "1.2\n",
            "97.23000526428223%\n",
            "1.3\n",
            "96.24000191688538%\n",
            "1.4\n",
            "95.65000534057617%\n",
            "Accuracy on Images sheared by factor:\n",
            "0.1\n",
            "97.55000472068787%\n",
            "0.2\n",
            "96.17000222206116%\n",
            "0.4\n",
            "78.55000495910645%\n",
            "0.5\n",
            "64.26000595092773%\n"
          ]
        }
      ],
      "source": [
        "test_invariance(srn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFoesQ-emZpU",
        "outputId": "4803a520-346f-43d0-ee9d-35ee14dc7505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 10s 32ms/step - loss: 0.0554 - categorical_accuracy: 0.9907\n",
            "[0.05539818853139877, 0.9907000660896301]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 0.0690 - categorical_accuracy: 0.9877\n",
            "[0.06896796077489853, 0.9877000451087952]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 0.0913 - categorical_accuracy: 0.9801\n",
            "[0.0913410410284996, 0.9801000356674194]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.2153 - categorical_accuracy: 0.9480\n",
            "[0.21528370678424835, 0.9480000734329224]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.7104 - categorical_accuracy: 0.8335\n",
            "[0.7104433178901672, 0.8335000276565552]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 3.6673 - categorical_accuracy: 0.3814\n",
            "[3.667268991470337, 0.3814000189304352]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 6.2965 - categorical_accuracy: 0.1795\n",
            "[6.296536445617676, 0.179500013589859]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.0908 - categorical_accuracy: 0.9793\n",
            "[0.09081210196018219, 0.9793000221252441]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.1046 - categorical_accuracy: 0.9750\n",
            "[0.10463044047355652, 0.9750000238418579]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.1363 - categorical_accuracy: 0.9668\n",
            "[0.1363106220960617, 0.9668000340461731]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.1665 - categorical_accuracy: 0.9563\n",
            "[0.166467547416687, 0.9563000202178955]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 1.2380 - categorical_accuracy: 0.6497\n",
            "[1.2380186319351196, 0.6497000455856323]\n",
            "313/313 [==============================] - 9s 29ms/step - loss: 0.0837 - categorical_accuracy: 0.9816\n",
            "[0.08374039083719254, 0.9816000461578369]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 0.1415 - categorical_accuracy: 0.9656\n",
            "[0.14152760803699493, 0.9656000733375549]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.8472 - categorical_accuracy: 0.7900\n",
            "[0.8471866846084595, 0.7900000214576721]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 1.6600 - categorical_accuracy: 0.6459\n",
            "[1.6599652767181396, 0.6459000110626221]\n"
          ]
        }
      ],
      "source": [
        "test_invariance(m64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-12T23:22:39.496957Z",
          "iopub.status.busy": "2023-12-12T23:22:39.496226Z",
          "iopub.status.idle": "2023-12-12T23:22:39.501275Z",
          "shell.execute_reply": "2023-12-12T23:22:39.500216Z",
          "shell.execute_reply.started": "2023-12-12T23:22:39.496925Z"
        },
        "id": "yiE3XYELmZpV"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T21:13:22.399956Z",
          "iopub.status.busy": "2023-11-20T21:13:22.399159Z",
          "iopub.status.idle": "2023-11-20T21:13:22.405128Z",
          "shell.execute_reply": "2023-11-20T21:13:22.40402Z",
          "shell.execute_reply.started": "2023-11-20T21:13:22.399904Z"
        },
        "id": "w5WzFRO8mZpV"
      },
      "outputs": [],
      "source": [
        "def edim(inp,axis=-1):\n",
        "    return tf.expand_dims(inp,axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sE5-LIzmZpa"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def test_invariance(model):\n",
        "  print('original accuracy')\n",
        "  print(str(model.evaluate(testXA,testY,verbose=0)[1]*100)+'%')\n",
        "  print('Accuracy on Images scaled by factor:')\n",
        "  print('1.1')\n",
        "  print(str(affine_scale(model,1.1)[1]*100)+'%')\n",
        "  print('1.2')\n",
        "  print(str(affine_scale(model,1.2)[1]*100)+'%')\n",
        "  print('1.3')\n",
        "  print(str(affine_scale(model,1.3)[1]*100)+'%')\n",
        "  print('1.4')\n",
        "  print(str(affine_scale(model,1.4)[1]*100)+'%')\n",
        "  affine_scale(model,2.0)\n",
        "  print('Accuracy on Images sheared by factor:')\n",
        "  print('0.1')\n",
        "  print(str(affine_shear(model,0.1)[1]*100)+'%')\n",
        "  print('0.2')\n",
        "  print(str(affine_shear(model,0.2)[1]*100)+'%')\n",
        "  print('0.4')\n",
        "  print(str(affine_shear(model,0.4)[1]*100)+'%')\n",
        "  print('0.5')\n",
        "  print(str(affine_shear(model,0.5)[1]*100)+'%')\n",
        "\n",
        "def affine_rot(model,theta):\n",
        "    tr = tfa.image.rotate(testXA,(theta/180)*math.pi)\n",
        "    image2 = tr\n",
        "    #print(model.evaluate(testXA,testY))\n",
        "    #indices = tf.where(tf.argmax(model(test_images),-1)==tf.argmax(test_labels),-1)\n",
        "    #print(model.evaluate(image2[indices],test_labels[indices]))\n",
        "    print(model.evaluate(tr,testY,verbose=0))\n",
        "\n",
        "def affine_scale(model,scale):\n",
        "    tr =  tfa.image.transform(testXA,[scale,0.0,0.0,0.0,scale,0,0,0])\n",
        "    image2=tr\n",
        "    return model.evaluate(image2,testY,verbose=0)\n",
        "\n",
        "def affine_shear(model,scale):\n",
        "    total_acc=0\n",
        "    n=0\n",
        "    #predictions1 = model.predict(testXA)\n",
        "    #image2 =np.array(image2)\n",
        "    tr = tfa.image.transform(testXA,[1.0,scale,0,scale,1.0,0,0,0])#tfa.image.shear_x(testXA,scale,replace=0.0) #\n",
        "    #print(model.evaluate(testXA,testY))\n",
        "    return model.evaluate(tr,testY,verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZacIsJAnmZpa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
