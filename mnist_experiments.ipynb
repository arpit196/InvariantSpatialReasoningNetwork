{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "uhi-YEmvO6eY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10,cifar100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuARlkvSO6eY",
        "outputId": "7a791ca7-62d1-4eec-d0ce-1ee8da0fec29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method MultiScaleConv.call of <ipynb.fs.full.SpatialReasoningModel.MultiScaleConv object at 0x0000023787B02460>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method MultiScaleConv.call of <ipynb.fs.full.SpatialReasoningModel.MultiScaleConv object at 0x0000023787B02460>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method SpatialReasoningLayer.call of <ipynb.fs.full.SpatialReasoningModel.SpatialReasoningLayer object at 0x00000237878D55B0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method SpatialReasoningLayer.call of <ipynb.fs.full.SpatialReasoningModel.SpatialReasoningLayer object at 0x00000237878D55B0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected EOF while parsing (<unknown>, line 1)\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        }
      ],
      "source": [
        "from . SpatialReasoningModel import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNrUf8gkO6eZ"
      },
      "outputs": [],
      "source": [
        "# example of loading the mnist dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn import datasets\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#from tensorflow.keras.datasets import cifar10,cifar100\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "def conc(*inp1):\n",
        "  return layers.Concatenate()(inp1)\n",
        "\n",
        "#!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "def mpool(psize,strides=2):\n",
        "  return MaxPooling2D(pool_size=psize,strides=strides,padding=\"SAME\")\n",
        "\n",
        "def apool(psize,strides=None):\n",
        "  if(strides is None):\n",
        "    return AveragePooling2D(pool_size=psize,padding=\"SAME\")\n",
        "  else:\n",
        "    return AveragePooling2D(pool_size=psize,strides=strides,padding=\"SAME\")\n",
        "\n",
        "def ln():\n",
        "  return layers.LayerNormalization()\n",
        "\n",
        "def bn():\n",
        "  return layers.BatchNormalization()\n",
        "\n",
        "def dense(size,act='relu'):\n",
        "  return Dense(size,activation=act)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow.keras.layers as layers\n",
        "filepath = 'my_best_model.hdf5'\n",
        "\n",
        "def bnconv(inp,units,kernel_size):\n",
        "  return BatchNormalization()(Conv2D(units,kernel_size,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005))(inp))\n",
        "\n",
        "def dense(units,act='relu'):\n",
        "  return layers.Dense(units,activation=act)\n",
        "\n",
        "def edim(inp,axis=-1):\n",
        "    return tf.expand_dims(inp,axis)\n",
        "\n",
        "def conv(inp,units,kernel_size):\n",
        "  return Conv2D(kernel_size,units,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005))(inp)\n",
        "\n",
        "def cutout(image,size):\n",
        "  return tfa.image.cutout(image,mask_size=(size,size))\n",
        "\n",
        "def LaplacianFilter(input_tensor):\n",
        "  filter = tf.constant([[[0.0,-1.0,0.0],[-1.0,4.0,-1.0],[0.0,-1.0,0.0]]])   #The filters for derivatives Cx, Cy ,[[0.0,-1.0],[1.0,0.0]]\n",
        "  filter = filter[:,:,:,tf.newaxis]\n",
        "  filter = tf.transpose(filter,[1,2,0,3])\n",
        "  print(filter.shape)\n",
        "  tf.print(filter[:,:,1,1])\n",
        "  out = tf.nn.conv2d(input_tensor,filters=filter,padding=\"SAME\",strides=[1, 1, 1, 1])  #basic convolution operation in tensorflow, the derivative filters are applied with stride 3\n",
        "  return out\n",
        "\n",
        "\n",
        "def LocalCurvature(input_tensor,scale=1,edge=True):\n",
        "  filter = tf.constant([[[-1.0,0.0],[0.0,0.0]],[[1.0,0.0],[1.0,-1.0]]])   #The filters for derivatives Cx, Cy ,[[0.0,-1.0],[1.0,0.0]]\n",
        "  filter = tf.transpose(filter,[1,2,0])\n",
        "  filter = filter[:,:,tf.newaxis] #Expanding according to number of input channels\n",
        "  filter = tf.tile(filter,[1,1,input_tensor.shape[-1],1])\n",
        "  filter2 = tf.constant([[[0.0,0.0],[0.0,0.0]],[[1.0,0.0],[0.0,1.0]]])   #The filters for derivatives Cx, Cy ,[[0.0,1.0],[0.0,0.0]]\n",
        "  filter2 = tf.transpose(filter2,[1,2,0])\n",
        "\n",
        "  filter2 = filter2[:,:,tf.newaxis]\n",
        "  filter2 = tf.tile(filter2,[1,1,input_tensor.shape[-1],1])                                         #Expanding according to number of input channels\n",
        "  filter3 = tf.constant([[[1.0,0.0],[0.0,0.0]],[[0.0,0.0],[1.0,0.0]]])   #The filters for derivatives Cx, Cy ,[[0.0,0.0],[1.0,0.0]\n",
        "  filter3 = tf.transpose(filter3,[1,2,0])\n",
        "  filter3 = filter3[:,:,tf.newaxis]\n",
        "  filter3 = tf.tile(filter3,[1,1,input_tensor.shape[-1],1])\n",
        "  out2 = tf.nn.conv2d(input_tensor, filters=filter2,padding=\"SAME\",dilations=scale,strides=[1, 1, 1, 1])\n",
        "  out3 = tf.nn.conv2d(input_tensor, filters=filter3,padding=\"SAME\",dilations=scale,strides=[1, 1, 1, 1])\n",
        "  out = tf.math.abs(tf.nn.conv2d(input_tensor,filters=filter,dilations=scale,padding=\"SAME\",strides=[1, 1, 1, 1]))  #basic convolution operation in tensorflow, the derivative filters are applied with stride 3\n",
        "  if edge:\n",
        "    return tf.where(tf.abs(out2*out3)>0,out,0.0)\n",
        "  else:\n",
        "    return out\n",
        "\n",
        "def LocalCurvature(input_tensor,scale=1,edge=True):\n",
        "  filter = tf.constant([[[-1.0,0.0],[1.0,0.0]],[[0.0,0.0],[1.0,-1.0]],[[0.0,-1.0],[1.0,0.0]]])   #The filters for derivatives Cx, Cy\n",
        "  filter = filter[:,:,tf.newaxis]                                         #Expanding according to number of input channels\n",
        "  filter = tf.tile(filter,[1,1,1,1])\n",
        "  filter2 = tf.constant([[[0.0,0.0],[1.0,0.0]],[[0.0,0.0],[0.0,1.0]],[[0.0,1.0],[0.0,0.0]]])   #The filters for derivatives Cx, Cy\n",
        "  filter2 = filter2[:,:,tf.newaxis]                                         #Expanding according to number of input channels\n",
        "  filter3 = tf.constant([[[1.0,0.0],[0.0,0.0]],[[0.0,0.0],[1.0,0.0]],[[0.0,0.0],[1.0,0.0]]])   #The filters for derivatives Cx, Cy\n",
        "  filter3 = filter3[:,:,tf.newaxis]\n",
        "  filter2 = tf.tile(filter2,[1,1,1,1])\n",
        "  out2 = tf.nn.conv2d(input_tensor, filters=filter2,padding=\"SAME\",dilations=scale,strides=[1, 1, 1, 1])\n",
        "  out3 = tf.nn.conv2d(input_tensor, filters=filter3,padding=\"SAME\",dilations=scale,strides=[1, 1, 1, 1])\n",
        "  out = tf.math.abs(tf.nn.conv2d(input_tensor,filters=filter,dilations=scale,padding=\"SAME\",strides=[1, 1, 1, 1]))  #basic convolution operation in tensorflow, the derivative filters are applied with stride 3\n",
        "  if edge:\n",
        "    return tf.where(tf.abs(out2*out3)>0,out,0.0)\n",
        "  else:\n",
        "    return out\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_imagecont(image, label,image_size=28):\n",
        "  image = tf.convert_to_tensor(image)\n",
        "  image = tf.image.resize(image, [image_size,image_size])\n",
        "  image1 = tfa.image.gaussian_filter2d(image, (2,2),3)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  #image = image/255.0\n",
        "  positionsx1 = tf.range(start=0, limit=image_size, delta=float(1),dtype=tf.float32)\n",
        "  positionsy1 = tf.range(start=0, limit=image_size, delta=float(1),dtype=tf.float32)\n",
        "  positionsx1 = tf.expand_dims(tf.tile(tf.expand_dims(positionsx1,0),[image_size,1]),-1)\n",
        "  positionsy1 = tf.expand_dims(tf.tile(tf.transpose(tf.expand_dims(positionsy1,0)),[1,image_size]),-1)\n",
        "  positions11 = tf.concat([positionsx1,positionsy1],-1); positions11 = tf.tile(positions11[tf.newaxis,:,:,:],[image.shape[0],1,1,1])\n",
        "  u = tf.image.sobel_edges(image1)\n",
        "  angle = tf.where(u[:,:,:,0,0]!=0,tf.atan2(u[:,:,:,0,1],u[:,:,:,0,0]),0)\n",
        "  angle = angle[:,:,:,tf.newaxis]\n",
        "  #u = tf.reshape(u,[-1,image_size,image_size,6])\n",
        "\n",
        "  image = tf.concat([image,angle/3.14,positions11],-1)\n",
        "  #image = tf.concat([u[tf.newaxis,:,:,:],angle,positions11[tf.newaxis,:]],-1)\n",
        "  #image=tf.squeeze(image,0)\n",
        "  return image, label\n",
        "\n",
        "labeled_batch_size=75\n",
        "num_epochs = 60\n",
        "batch_size = 30  # Corresponds to 200 steps per epoch\n",
        "width = 32\n",
        "temperature = 1\n",
        "learning_rate=0.0001\n",
        "#lr_drop=20\n",
        "\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "#select = tf.convert_to_tensor(select,dtype=tf.float32)\n",
        "\n",
        "def train(model,path,epochs=100):\n",
        "    checkpoint1 = ModelCheckpoint(filepath='./'+path,save_format=tf,monitor='val_loss',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(0.1,momentum=0.9),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics='categorical_accuracy')\n",
        "    model.fit(trainXA, trainY, epochs=epochs, batch_size=200, validation_data=(testXA, testY), callbacks=[checkpoint1, reduce_lr], verbose=1)\n",
        "\n",
        "learning_rate = 0.1\n",
        "lr_drop=15\n",
        "def lr_scheduler(epoch):\n",
        "        return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "\n",
        "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "checkpoint_gen = ModelCheckpoint(filepath='./'+'generative_cifar_2',save_format=tf,monitor='val_val_loss',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "image_size=28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XML1GcJqO6eZ"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "\t# load dataset\n",
        "  (trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "  trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "  testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "  return trainX, trainY, testX, testY\n",
        "\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "  train_norm = train.astype('float32')\n",
        "  test_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "  train_norm = train_norm / 255.0\n",
        "  test_norm = test_norm / 255.0\n",
        "  normalization = layers.Normalization()\n",
        "  normalization.adapt(train_norm)\n",
        "  #train_norm=normalization(train_norm)\n",
        "  #test_norm=normalization(test_norm)\n",
        "  return train_norm, test_norm\n",
        "\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "\t# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)\n",
        "trainXA,trainY = preprocess_imagecont(trainX,trainY)\n",
        "testXA,testY = preprocess_imagecont(testX,testY)\n",
        "#train_images_res = tf.image.resize(trainXA,[48,48],method='bilinear')\n",
        "#test_images_res = tf.image.resize(testXA,[48,48],method='bilinear')\n",
        "\n",
        "trainY = tf.keras.utils.to_categorical(trainY,num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(testY,num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-UtJA9_O6ea"
      },
      "outputs": [],
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(name='lin_w',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(name='lin_b',\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        out = (tf.matmul(inputs, self.w) - self.b)**2\n",
        "        #self.add_loss(0.001*tf.reduce_mean(tf.where(out>0.0,tf.reduce_sum(self.w,0),0.0)))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB-TrKWxO6ea"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WWmwePmO6ea"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001\n",
        "lr_drop=20\n",
        "def lr_scheduler(epoch):\n",
        "        return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "\n",
        "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "checkpoint_gen = ModelCheckpoint(filepath='./'+'generative_cifar_2',save_format=tf,monitor='val_val_loss',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lR4L2sdO6ea"
      },
      "outputs": [],
      "source": [
        "class MultiScaleConv(keras.layers.Layer):\n",
        "  def __init__(self,kernel_size,regularizer=1,filters=256):\n",
        "    self.kernel_size = kernel_size\n",
        "    self.filters = filters\n",
        "    self.regularizer=regularizer\n",
        "    super(MultiScaleConv,self).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "        shape = list(self.kernel_size) + [input_shape[-1], self.filters]\n",
        "        if self.regularizer is not None:\n",
        "            self.kernel = self.add_weight(name='kernel', shape=shape,\n",
        "                                          initializer='glorot_normal',regularizer=tf.keras.regularizers.L2(0.0004),\n",
        "                                         trainable=True)\n",
        "        else:\n",
        "            self.kernel = self.add_weight(name='kernel', shape=shape,\n",
        "                                          initializer='glorot_uniform',trainable=True)\n",
        "        super(MultiScaleConv, self).build(input_shape)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    dup_cols=self.kernel\n",
        "    l = layers.Maximum()([tf.nn.atrous_conv2d(inputs,rate=1,filters=dup_cols,padding=\"SAME\"),tf.nn.atrous_conv2d(inputs,rate=2,filters=dup_cols,padding=\"SAME\")])\n",
        "    l = tf.keras.activations.relu(l)\n",
        "    return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y70nY2rQO6ea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avGf9vViO6ea"
      },
      "outputs": [],
      "source": [
        "def train(model,path,epochs=80):\n",
        "  checkpoint1 = ModelCheckpoint(filepath='./'+path,save_format=tf,monitor='val_categorical_accuracy',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='max')\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics='categorical_accuracy')\n",
        "  model.fit(trainXA,trainY, epochs=epochs, batch_size=30, validation_data=(testXA,testY), callbacks=[reduce_lr,checkpoint1], verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9dxFXpTO6eb"
      },
      "outputs": [],
      "source": [
        "class RBFLayer(keras.layers.Layer):\n",
        "    def __init__(self, clusters=200, activation='none',**kwargs):\n",
        "        super(RBFLayer, self).__init__(**kwargs)\n",
        "        self.clusters = clusters; self.bn = layers.BatchNormalization();\n",
        "        self.activation=activation\n",
        "\n",
        "    def initialize_centers(self):\n",
        "        n_centers = shape[0]\n",
        "        self.mu = km.cluster_centers\n",
        "        return km.cluster_centers_\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.mu = self.add_weight(name='mu',\n",
        "                                  shape=([input_shape[-1],self.clusters]),\n",
        "                                  initializer='uniform',\n",
        "                                  trainable=True)\n",
        "\n",
        "        self.t = tf.Variable(name='threshold',initial_value=0.6,trainable=True)\n",
        "\n",
        "        self.c = self.add_weight(name='gamma',\n",
        "                                  shape=([self.clusters]),\n",
        "                                  initializer='uniform',\n",
        "                                  trainable=True)\n",
        "        super(RBFLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        diff = (K.expand_dims(inputs) - self.mu)#\n",
        "        var = (tf.math.reduce_mean(diff,[0,1,2],keepdims=True)+1e-8)\n",
        "        l2 = (diff**2)#/var\n",
        "        l2 = tf.reduce_sum(l2,-2)\n",
        "        #tf.print(l2)\n",
        "        if self.activation=='softmax':\n",
        "            res = tf.keras.activations.softmax(-l2)\n",
        "        else:\n",
        "            res = 1/(1.0+(l2/self.c**2))\n",
        "        #res = 1/(1.0+0.5*l2)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-04T13:23:42.802818Z",
          "iopub.status.busy": "2024-02-04T13:23:42.802439Z",
          "iopub.status.idle": "2024-02-04T14:23:43.662753Z",
          "shell.execute_reply": "2024-02-04T14:23:43.660923Z",
          "shell.execute_reply.started": "2024-02-04T13:23:42.802786Z"
        },
        "id": "Oy3_l5SoO6eb",
        "outputId": "c49f40c3-a0f4-43f3-fa68-aef5f8ed9eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 7, 7, 840)\n",
            "inputsshape(None, 7, 7, 13440)\n",
            "(None, 1240)\n",
            "Epoch 1/80\n",
            "(30, 7, 7, 840)\n",
            "inputsshape(30, 7, 7, 13440)\n",
            "(30, 1240)\n",
            "(30, 7, 7, 840)\n",
            "inputsshape(30, 7, 7, 13440)\n",
            "(30, 1240)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-04 13:23:45.657333: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/tf.where_4/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1998/2000 [============================>.] - ETA: 0s - loss: 1.2575 - categorical_accuracy: 0.9335(None, 7, 7, 840)\n",
            "inputsshape(None, 7, 7, 13440)\n",
            "(None, 1240)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-04 13:24:36.997778: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/tf.where_4/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.96500, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 57s 26ms/step - loss: 1.2572 - categorical_accuracy: 0.9335 - val_loss: 1.0078 - val_categorical_accuracy: 0.9650 - lr: 1.0000e-04\n",
            "Epoch 2/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.8909 - categorical_accuracy: 0.9711\n",
            "Epoch 2: val_categorical_accuracy improved from 0.96500 to 0.97110, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 26ms/step - loss: 0.8909 - categorical_accuracy: 0.9710 - val_loss: 0.7950 - val_categorical_accuracy: 0.9711 - lr: 1.0000e-04\n",
            "Epoch 3/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.6857 - categorical_accuracy: 0.9773\n",
            "Epoch 3: val_categorical_accuracy improved from 0.97110 to 0.98020, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 26ms/step - loss: 0.6858 - categorical_accuracy: 0.9773 - val_loss: 0.5925 - val_categorical_accuracy: 0.9802 - lr: 1.0000e-04\n",
            "Epoch 4/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.5167 - categorical_accuracy: 0.9813\n",
            "Epoch 4: val_categorical_accuracy improved from 0.98020 to 0.98060, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.5167 - categorical_accuracy: 0.9813 - val_loss: 0.4539 - val_categorical_accuracy: 0.9806 - lr: 1.0000e-04\n",
            "Epoch 5/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.3845 - categorical_accuracy: 0.9853\n",
            "Epoch 5: val_categorical_accuracy did not improve from 0.98060\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.3844 - categorical_accuracy: 0.9853 - val_loss: 0.3771 - val_categorical_accuracy: 0.9765 - lr: 1.0000e-04\n",
            "Epoch 6/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.2999 - categorical_accuracy: 0.9862\n",
            "Epoch 6: val_categorical_accuracy improved from 0.98060 to 0.98460, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.2999 - categorical_accuracy: 0.9862 - val_loss: 0.2811 - val_categorical_accuracy: 0.9846 - lr: 1.0000e-04\n",
            "Epoch 7/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.2417 - categorical_accuracy: 0.9878\n",
            "Epoch 7: val_categorical_accuracy improved from 0.98460 to 0.98490, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.2418 - categorical_accuracy: 0.9878 - val_loss: 0.2348 - val_categorical_accuracy: 0.9849 - lr: 1.0000e-04\n",
            "Epoch 8/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1979 - categorical_accuracy: 0.9901\n",
            "Epoch 8: val_categorical_accuracy did not improve from 0.98490\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1979 - categorical_accuracy: 0.9901 - val_loss: 0.2532 - val_categorical_accuracy: 0.9694 - lr: 1.0000e-04\n",
            "Epoch 9/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.1684 - categorical_accuracy: 0.9912\n",
            "Epoch 9: val_categorical_accuracy improved from 0.98490 to 0.98820, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1683 - categorical_accuracy: 0.9912 - val_loss: 0.1672 - val_categorical_accuracy: 0.9882 - lr: 1.0000e-04\n",
            "Epoch 10/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1435 - categorical_accuracy: 0.9922\n",
            "Epoch 10: val_categorical_accuracy did not improve from 0.98820\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.1435 - categorical_accuracy: 0.9923 - val_loss: 0.1670 - val_categorical_accuracy: 0.9830 - lr: 1.0000e-04\n",
            "Epoch 11/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1263 - categorical_accuracy: 0.9929\n",
            "Epoch 11: val_categorical_accuracy improved from 0.98820 to 0.98850, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1263 - categorical_accuracy: 0.9929 - val_loss: 0.1378 - val_categorical_accuracy: 0.9885 - lr: 1.0000e-04\n",
            "Epoch 12/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.1104 - categorical_accuracy: 0.9943\n",
            "Epoch 12: val_categorical_accuracy did not improve from 0.98850\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1104 - categorical_accuracy: 0.9943 - val_loss: 0.1246 - val_categorical_accuracy: 0.9879 - lr: 1.0000e-04\n",
            "Epoch 13/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1005 - categorical_accuracy: 0.9942\n",
            "Epoch 13: val_categorical_accuracy did not improve from 0.98850\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.1005 - categorical_accuracy: 0.9942 - val_loss: 0.1247 - val_categorical_accuracy: 0.9856 - lr: 1.0000e-04\n",
            "Epoch 14/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0907 - categorical_accuracy: 0.9947\n",
            "Epoch 14: val_categorical_accuracy did not improve from 0.98850\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0907 - categorical_accuracy: 0.9947 - val_loss: 0.1149 - val_categorical_accuracy: 0.9867 - lr: 1.0000e-04\n",
            "Epoch 15/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0836 - categorical_accuracy: 0.9953\n",
            "Epoch 15: val_categorical_accuracy did not improve from 0.98850\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0836 - categorical_accuracy: 0.9953 - val_loss: 0.1388 - val_categorical_accuracy: 0.9790 - lr: 1.0000e-04\n",
            "Epoch 16/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0684 - categorical_accuracy: 0.9983\n",
            "Epoch 16: val_categorical_accuracy improved from 0.98850 to 0.99190, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 26ms/step - loss: 0.0684 - categorical_accuracy: 0.9983 - val_loss: 0.0869 - val_categorical_accuracy: 0.9919 - lr: 5.0000e-05\n",
            "Epoch 17/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0605 - categorical_accuracy: 0.9990\n",
            "Epoch 17: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0605 - categorical_accuracy: 0.9990 - val_loss: 0.0854 - val_categorical_accuracy: 0.9904 - lr: 5.0000e-05\n",
            "Epoch 18/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0563 - categorical_accuracy: 0.9986\n",
            "Epoch 18: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0563 - categorical_accuracy: 0.9986 - val_loss: 0.0802 - val_categorical_accuracy: 0.9901 - lr: 5.0000e-05\n",
            "Epoch 19/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0513 - categorical_accuracy: 0.9990\n",
            "Epoch 19: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0513 - categorical_accuracy: 0.9990 - val_loss: 0.0781 - val_categorical_accuracy: 0.9906 - lr: 5.0000e-05\n",
            "Epoch 20/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0477 - categorical_accuracy: 0.9988\n",
            "Epoch 20: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0477 - categorical_accuracy: 0.9988 - val_loss: 0.0755 - val_categorical_accuracy: 0.9902 - lr: 5.0000e-05\n",
            "Epoch 21/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0451 - categorical_accuracy: 0.9987\n",
            "Epoch 21: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0451 - categorical_accuracy: 0.9987 - val_loss: 0.0709 - val_categorical_accuracy: 0.9906 - lr: 5.0000e-05\n",
            "Epoch 22/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0426 - categorical_accuracy: 0.9988\n",
            "Epoch 22: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0426 - categorical_accuracy: 0.9988 - val_loss: 0.0757 - val_categorical_accuracy: 0.9894 - lr: 5.0000e-05\n",
            "Epoch 23/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0408 - categorical_accuracy: 0.9988\n",
            "Epoch 23: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0408 - categorical_accuracy: 0.9988 - val_loss: 0.0687 - val_categorical_accuracy: 0.9908 - lr: 5.0000e-05\n",
            "Epoch 24/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0392 - categorical_accuracy: 0.9987\n",
            "Epoch 24: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0392 - categorical_accuracy: 0.9987 - val_loss: 0.0661 - val_categorical_accuracy: 0.9911 - lr: 5.0000e-05\n",
            "Epoch 25/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0380 - categorical_accuracy: 0.9985\n",
            "Epoch 25: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0380 - categorical_accuracy: 0.9985 - val_loss: 0.0740 - val_categorical_accuracy: 0.9883 - lr: 5.0000e-05\n",
            "Epoch 26/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0359 - categorical_accuracy: 0.9989\n",
            "Epoch 26: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0359 - categorical_accuracy: 0.9989 - val_loss: 0.0748 - val_categorical_accuracy: 0.9877 - lr: 5.0000e-05\n",
            "Epoch 27/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0338 - categorical_accuracy: 0.9991\n",
            "Epoch 27: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0338 - categorical_accuracy: 0.9991 - val_loss: 0.0615 - val_categorical_accuracy: 0.9902 - lr: 5.0000e-05\n",
            "Epoch 28/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0343 - categorical_accuracy: 0.9987\n",
            "Epoch 28: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0343 - categorical_accuracy: 0.9987 - val_loss: 0.0620 - val_categorical_accuracy: 0.9903 - lr: 5.0000e-05\n",
            "Epoch 29/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0311 - categorical_accuracy: 0.9994\n",
            "Epoch 29: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0311 - categorical_accuracy: 0.9994 - val_loss: 0.0655 - val_categorical_accuracy: 0.9894 - lr: 5.0000e-05\n",
            "Epoch 30/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0318 - categorical_accuracy: 0.9989\n",
            "Epoch 30: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0318 - categorical_accuracy: 0.9989 - val_loss: 0.0573 - val_categorical_accuracy: 0.9905 - lr: 5.0000e-05\n",
            "Epoch 31/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0274 - categorical_accuracy: 0.9998\n",
            "Epoch 31: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0274 - categorical_accuracy: 0.9998 - val_loss: 0.0528 - val_categorical_accuracy: 0.9917 - lr: 2.5000e-05\n",
            "Epoch 32/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0255 - categorical_accuracy: 0.9998\n",
            "Epoch 32: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0255 - categorical_accuracy: 0.9998 - val_loss: 0.0600 - val_categorical_accuracy: 0.9889 - lr: 2.5000e-05\n",
            "Epoch 33/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0237 - categorical_accuracy: 0.9999\n",
            "Epoch 33: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0237 - categorical_accuracy: 0.9999 - val_loss: 0.0481 - val_categorical_accuracy: 0.9919 - lr: 2.5000e-05\n",
            "Epoch 34/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0226 - categorical_accuracy: 0.9997\n",
            "Epoch 34: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0226 - categorical_accuracy: 0.9997 - val_loss: 0.0471 - val_categorical_accuracy: 0.9910 - lr: 2.5000e-05\n",
            "Epoch 35/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0213 - categorical_accuracy: 0.9999\n",
            "Epoch 35: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0213 - categorical_accuracy: 0.9999 - val_loss: 0.0539 - val_categorical_accuracy: 0.9891 - lr: 2.5000e-05\n",
            "Epoch 36/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0203 - categorical_accuracy: 0.9999\n",
            "Epoch 36: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0203 - categorical_accuracy: 0.9999 - val_loss: 0.0476 - val_categorical_accuracy: 0.9906 - lr: 2.5000e-05\n",
            "Epoch 37/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0192 - categorical_accuracy: 0.9998\n",
            "Epoch 37: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0192 - categorical_accuracy: 0.9998 - val_loss: 0.0498 - val_categorical_accuracy: 0.9901 - lr: 2.5000e-05\n",
            "Epoch 38/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0183 - categorical_accuracy: 0.9999\n",
            "Epoch 38: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0183 - categorical_accuracy: 0.9999 - val_loss: 0.0455 - val_categorical_accuracy: 0.9910 - lr: 2.5000e-05\n",
            "Epoch 39/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0181 - categorical_accuracy: 0.9997\n",
            "Epoch 39: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0181 - categorical_accuracy: 0.9997 - val_loss: 0.0507 - val_categorical_accuracy: 0.9901 - lr: 2.5000e-05\n",
            "Epoch 40/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0176 - categorical_accuracy: 0.9997\n",
            "Epoch 40: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0176 - categorical_accuracy: 0.9997 - val_loss: 0.0473 - val_categorical_accuracy: 0.9914 - lr: 2.5000e-05\n",
            "Epoch 41/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0164 - categorical_accuracy: 0.9999\n",
            "Epoch 41: val_categorical_accuracy did not improve from 0.99190\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0164 - categorical_accuracy: 0.9999 - val_loss: 0.0522 - val_categorical_accuracy: 0.9891 - lr: 2.5000e-05\n",
            "Epoch 42/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0164 - categorical_accuracy: 0.9998\n",
            "Epoch 42: val_categorical_accuracy improved from 0.99190 to 0.99220, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0164 - categorical_accuracy: 0.9998 - val_loss: 0.0444 - val_categorical_accuracy: 0.9922 - lr: 2.5000e-05\n",
            "Epoch 43/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0163 - categorical_accuracy: 0.9996\n",
            "Epoch 43: val_categorical_accuracy did not improve from 0.99220\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0163 - categorical_accuracy: 0.9997 - val_loss: 0.0428 - val_categorical_accuracy: 0.9919 - lr: 2.5000e-05\n",
            "Epoch 44/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0159 - categorical_accuracy: 0.9997\n",
            "Epoch 44: val_categorical_accuracy improved from 0.99220 to 0.99230, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0159 - categorical_accuracy: 0.9997 - val_loss: 0.0415 - val_categorical_accuracy: 0.9923 - lr: 2.5000e-05\n",
            "Epoch 45/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0155 - categorical_accuracy: 0.9998\n",
            "Epoch 45: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0155 - categorical_accuracy: 0.9998 - val_loss: 0.0496 - val_categorical_accuracy: 0.9900 - lr: 2.5000e-05\n",
            "Epoch 46/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0144 - categorical_accuracy: 1.0000\n",
            "Epoch 46: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0144 - categorical_accuracy: 1.0000 - val_loss: 0.0406 - val_categorical_accuracy: 0.9917 - lr: 1.2500e-05\n",
            "Epoch 47/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0136 - categorical_accuracy: 1.0000\n",
            "Epoch 47: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0136 - categorical_accuracy: 1.0000 - val_loss: 0.0419 - val_categorical_accuracy: 0.9915 - lr: 1.2500e-05\n",
            "Epoch 48/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0131 - categorical_accuracy: 1.0000\n",
            "Epoch 48: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0131 - categorical_accuracy: 1.0000 - val_loss: 0.0376 - val_categorical_accuracy: 0.9917 - lr: 1.2500e-05\n",
            "Epoch 49/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0123 - categorical_accuracy: 1.0000\n",
            "Epoch 49: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0123 - categorical_accuracy: 1.0000 - val_loss: 0.0386 - val_categorical_accuracy: 0.9916 - lr: 1.2500e-05\n",
            "Epoch 50/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0117 - categorical_accuracy: 1.0000\n",
            "Epoch 50: val_categorical_accuracy did not improve from 0.99230\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0117 - categorical_accuracy: 1.0000 - val_loss: 0.0408 - val_categorical_accuracy: 0.9904 - lr: 1.2500e-05\n",
            "Epoch 51/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0112 - categorical_accuracy: 1.0000\n",
            "Epoch 51: val_categorical_accuracy improved from 0.99230 to 0.99250, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0112 - categorical_accuracy: 1.0000 - val_loss: 0.0346 - val_categorical_accuracy: 0.9925 - lr: 1.2500e-05\n",
            "Epoch 52/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0107 - categorical_accuracy: 1.0000\n",
            "Epoch 52: val_categorical_accuracy did not improve from 0.99250\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0107 - categorical_accuracy: 1.0000 - val_loss: 0.0369 - val_categorical_accuracy: 0.9911 - lr: 1.2500e-05\n",
            "Epoch 53/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0103 - categorical_accuracy: 1.0000\n",
            "Epoch 53: val_categorical_accuracy did not improve from 0.99250\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0103 - categorical_accuracy: 1.0000 - val_loss: 0.0343 - val_categorical_accuracy: 0.9922 - lr: 1.2500e-05\n",
            "Epoch 54/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0101 - categorical_accuracy: 1.0000\n",
            "Epoch 54: val_categorical_accuracy improved from 0.99250 to 0.99260, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0101 - categorical_accuracy: 1.0000 - val_loss: 0.0337 - val_categorical_accuracy: 0.9926 - lr: 1.2500e-05\n",
            "Epoch 55/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0096 - categorical_accuracy: 1.0000\n",
            "Epoch 55: val_categorical_accuracy improved from 0.99260 to 0.99290, saving model to ./2_lyr_srn_1conv_bn\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0096 - categorical_accuracy: 1.0000 - val_loss: 0.0345 - val_categorical_accuracy: 0.9929 - lr: 1.2500e-05\n",
            "Epoch 56/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0095 - categorical_accuracy: 0.9999\n",
            "Epoch 56: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0095 - categorical_accuracy: 0.9999 - val_loss: 0.0380 - val_categorical_accuracy: 0.9911 - lr: 1.2500e-05\n",
            "Epoch 57/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0092 - categorical_accuracy: 1.0000\n",
            "Epoch 57: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0092 - categorical_accuracy: 1.0000 - val_loss: 0.0352 - val_categorical_accuracy: 0.9920 - lr: 1.2500e-05\n",
            "Epoch 58/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0087 - categorical_accuracy: 1.0000\n",
            "Epoch 58: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0087 - categorical_accuracy: 1.0000 - val_loss: 0.0341 - val_categorical_accuracy: 0.9921 - lr: 1.2500e-05\n",
            "Epoch 59/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0087 - categorical_accuracy: 1.0000\n",
            "Epoch 59: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0087 - categorical_accuracy: 1.0000 - val_loss: 0.0385 - val_categorical_accuracy: 0.9911 - lr: 1.2500e-05\n",
            "Epoch 60/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0086 - categorical_accuracy: 1.0000\n",
            "Epoch 60: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0086 - categorical_accuracy: 1.0000 - val_loss: 0.0332 - val_categorical_accuracy: 0.9922 - lr: 1.2500e-05\n",
            "Epoch 61/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0081 - categorical_accuracy: 1.0000\n",
            "Epoch 61: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0081 - categorical_accuracy: 1.0000 - val_loss: 0.0344 - val_categorical_accuracy: 0.9915 - lr: 6.2500e-06\n",
            "Epoch 62/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0079 - categorical_accuracy: 1.0000\n",
            "Epoch 62: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 52s 26ms/step - loss: 0.0079 - categorical_accuracy: 1.0000 - val_loss: 0.0368 - val_categorical_accuracy: 0.9912 - lr: 6.2500e-06\n",
            "Epoch 63/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0077 - categorical_accuracy: 1.0000\n",
            "Epoch 63: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0077 - categorical_accuracy: 1.0000 - val_loss: 0.0333 - val_categorical_accuracy: 0.9920 - lr: 6.2500e-06\n",
            "Epoch 64/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0073 - categorical_accuracy: 1.0000\n",
            "Epoch 64: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0073 - categorical_accuracy: 1.0000 - val_loss: 0.0322 - val_categorical_accuracy: 0.9919 - lr: 6.2500e-06\n",
            "Epoch 65/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0071 - categorical_accuracy: 1.0000\n",
            "Epoch 65: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0071 - categorical_accuracy: 1.0000 - val_loss: 0.0312 - val_categorical_accuracy: 0.9926 - lr: 6.2500e-06\n",
            "Epoch 66/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0068 - categorical_accuracy: 1.0000\n",
            "Epoch 66: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0068 - categorical_accuracy: 1.0000 - val_loss: 0.0314 - val_categorical_accuracy: 0.9926 - lr: 6.2500e-06\n",
            "Epoch 67/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0066 - categorical_accuracy: 1.0000\n",
            "Epoch 67: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0066 - categorical_accuracy: 1.0000 - val_loss: 0.0341 - val_categorical_accuracy: 0.9918 - lr: 6.2500e-06\n",
            "Epoch 68/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0064 - categorical_accuracy: 1.0000\n",
            "Epoch 68: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0064 - categorical_accuracy: 1.0000 - val_loss: 0.0309 - val_categorical_accuracy: 0.9927 - lr: 6.2500e-06\n",
            "Epoch 69/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0062 - categorical_accuracy: 1.0000\n",
            "Epoch 69: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 50s 25ms/step - loss: 0.0062 - categorical_accuracy: 1.0000 - val_loss: 0.0330 - val_categorical_accuracy: 0.9916 - lr: 6.2500e-06\n",
            "Epoch 70/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0060 - categorical_accuracy: 1.0000\n",
            "Epoch 70: val_categorical_accuracy did not improve from 0.99290\n",
            "2000/2000 [==============================] - 51s 25ms/step - loss: 0.0060 - categorical_accuracy: 1.0000 - val_loss: 0.0364 - val_categorical_accuracy: 0.9910 - lr: 6.2500e-06\n",
            "Epoch 71/80\n",
            "1998/2000 [============================>.] - ETA: 0s - loss: 0.0058 - categorical_accuracy: 1.0000"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m62,_ \u001b[38;5;241m=\u001b[39m AffineModel()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm62\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2_lyr_srn_1conv_bn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[21], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, path, epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m checkpoint1 \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpath,save_format\u001b[38;5;241m=\u001b[39mtf,monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                           save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m                            verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      5\u001b[0m                            save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                            mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[1;32m      8\u001b[0m   loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m   metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1777\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1778\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[1;32m   1790\u001b[0m     )\n\u001b[0;32m-> 1791\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1804\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1806\u001b[0m }\n\u001b[1;32m   1807\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2198\u001b[0m             ):\n\u001b[1;32m   2199\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2200\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4000\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   4002\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "m = SpatialRelationsModel()\n",
        "train(m,'2_lyr_srn_1conv_bn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHxKCBhpO6eb",
        "outputId": "157a1937-a9c9-49e5-a1dc-cfc5b685c38a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 13s 32ms/step - loss: 0.0554 - categorical_accuracy: 0.9907\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.05539818853139877, 0.9907000660896301]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m.evaluate(testXA,testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqKI5k-RO6eb"
      },
      "outputs": [],
      "source": [
        "m62,_ = SpatialRelationsModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-04T14:24:24.659292Z",
          "iopub.status.busy": "2024-02-04T14:24:24.658894Z",
          "iopub.status.idle": "2024-02-04T14:25:25.364533Z",
          "shell.execute_reply": "2024-02-04T14:25:25.363589Z",
          "shell.execute_reply.started": "2024-02-04T14:24:24.659258Z"
        },
        "id": "n8bLEqvWO6eb",
        "outputId": "0d09c14d-0aad-4463-f78e-1a45ae5cc842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0345 - categorical_accuracy: 0.9929\n",
            "[0.03445714712142944, 0.992900013923645]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0534 - categorical_accuracy: 0.9858\n",
            "[0.05338124930858612, 0.98580002784729]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0835 - categorical_accuracy: 0.9785\n",
            "[0.08345513790845871, 0.9785000085830688]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.2276 - categorical_accuracy: 0.9362\n",
            "[0.22759462893009186, 0.9362000226974487]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.7120 - categorical_accuracy: 0.8237\n",
            "[0.7120230197906494, 0.8237000107765198]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 3.3147 - categorical_accuracy: 0.4178\n",
            "[3.314708709716797, 0.41780000925064087]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 5.1703 - categorical_accuracy: 0.2414\n",
            "[5.17026948928833, 0.24140000343322754]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0672 - categorical_accuracy: 0.9834\n",
            "[0.06724955886602402, 0.9833999872207642]\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0805 - categorical_accuracy: 0.9769\n",
            "[0.08047843724489212, 0.9768999814987183]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.1175 - categorical_accuracy: 0.9667\n",
            "[0.11754981428384781, 0.96670001745224]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.1520 - categorical_accuracy: 0.9562\n",
            "[0.15203022956848145, 0.9562000036239624]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.0587 - categorical_accuracy: 0.6981\n",
            "[1.0587340593338013, 0.6980999708175659]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0595 - categorical_accuracy: 0.9843\n",
            "[0.05954614281654358, 0.9843000173568726]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.1228 - categorical_accuracy: 0.9642\n",
            "[0.12276379764080048, 0.9642000198364258]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.7141 - categorical_accuracy: 0.8028\n",
            "[0.714112401008606, 0.8027999997138977]\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.7828 - categorical_accuracy: 0.5902\n",
            "[1.7828409671783447, 0.5902000069618225]\n"
          ]
        }
      ],
      "source": [
        "m62.load_weights('./2_lyr_srn_1conv_bn')\n",
        "test_all(m62)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Invariance of Single layer SRN with 2 convolutional layers"
      ],
      "metadata": {
        "id": "8E8u_5jmQTOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB2lk3uZO6ec",
        "outputId": "c67a6863-fb80-407b-d267-f36d30a99d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2377b113e20>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m.load_weights('single_layer_2conv_srn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXxOzXg4O6ec",
        "outputId": "3842eb15-b73d-448b-867b-06cabb5e6995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 10s 32ms/step - loss: 0.0554 - categorical_accuracy: 0.9907\n",
            "[0.05539818853139877, 0.9907000660896301]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 0.0690 - categorical_accuracy: 0.9877\n",
            "[0.06896796077489853, 0.9877000451087952]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 0.0913 - categorical_accuracy: 0.9801\n",
            "[0.0913410410284996, 0.9801000356674194]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.2153 - categorical_accuracy: 0.9480\n",
            "[0.21528370678424835, 0.9480000734329224]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.7104 - categorical_accuracy: 0.8335\n",
            "[0.7104433178901672, 0.8335000276565552]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 3.6673 - categorical_accuracy: 0.3814\n",
            "[3.667268991470337, 0.3814000189304352]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 6.2965 - categorical_accuracy: 0.1795\n",
            "[6.296536445617676, 0.179500013589859]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.0908 - categorical_accuracy: 0.9793\n",
            "[0.09081210196018219, 0.9793000221252441]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.1046 - categorical_accuracy: 0.9750\n",
            "[0.10463044047355652, 0.9750000238418579]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.1363 - categorical_accuracy: 0.9668\n",
            "[0.1363106220960617, 0.9668000340461731]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.1665 - categorical_accuracy: 0.9563\n",
            "[0.166467547416687, 0.9563000202178955]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 1.2380 - categorical_accuracy: 0.6497\n",
            "[1.2380186319351196, 0.6497000455856323]\n",
            "313/313 [==============================] - 9s 29ms/step - loss: 0.0837 - categorical_accuracy: 0.9816\n",
            "[0.08374039083719254, 0.9816000461578369]\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 0.1415 - categorical_accuracy: 0.9656\n",
            "[0.14152760803699493, 0.9656000733375549]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.8472 - categorical_accuracy: 0.7900\n",
            "[0.8471866846084595, 0.7900000214576721]\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 1.6600 - categorical_accuracy: 0.6459\n",
            "[1.6599652767181396, 0.6459000110626221]\n"
          ]
        }
      ],
      "source": [
        "test_invariance(m64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T18:10:01.087932Z",
          "iopub.status.busy": "2024-02-02T18:10:01.087560Z",
          "iopub.status.idle": "2024-02-02T19:04:27.311468Z",
          "shell.execute_reply": "2024-02-02T19:04:27.309948Z",
          "shell.execute_reply.started": "2024-02-02T18:10:01.087902Z"
        },
        "id": "q7VubYfOO6ec",
        "outputId": "7e2d1cb6-ff71-4cf0-ddd0-cfee38637cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 7, 7, 840)\n",
            "inputsshape(None, 7, 7, 13440)\n",
            "lshape(None, 7, 7, 1240)\n",
            "(None, 1240)\n",
            "Epoch 1/80\n",
            "(30, 7, 7, 840)\n",
            "inputsshape(30, 7, 7, 13440)\n",
            "(30, 1240)\n",
            "(30, 7, 7, 840)\n",
            "inputsshape(30, 7, 7, 13440)\n",
            "(30, 1240)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-02 18:10:05.498872: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/tf.where_1/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.2377 - categorical_accuracy: 0.9349(None, 7, 7, 840)\n",
            "inputsshape(None, 7, 7, 13440)\n",
            "(None, 1240)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-02 18:11:01.768737: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/tf.where_1/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 1.01508, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 63s 28ms/step - loss: 1.2375 - categorical_accuracy: 0.9349 - val_loss: 1.0151 - val_categorical_accuracy: 0.9579\n",
            "Epoch 2/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.8713 - categorical_accuracy: 0.9717\n",
            "Epoch 2: val_loss improved from 1.01508 to 0.77095, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 56s 28ms/step - loss: 0.8713 - categorical_accuracy: 0.9717 - val_loss: 0.7709 - val_categorical_accuracy: 0.9724\n",
            "Epoch 3/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.6629 - categorical_accuracy: 0.9787\n",
            "Epoch 3: val_loss improved from 0.77095 to 0.56612, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.6628 - categorical_accuracy: 0.9787 - val_loss: 0.5661 - val_categorical_accuracy: 0.9832\n",
            "Epoch 4/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.4951 - categorical_accuracy: 0.9829\n",
            "Epoch 4: val_loss improved from 0.56612 to 0.43584, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.4951 - categorical_accuracy: 0.9829 - val_loss: 0.4358 - val_categorical_accuracy: 0.9819\n",
            "Epoch 5/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.3755 - categorical_accuracy: 0.9841\n",
            "Epoch 5: val_loss improved from 0.43584 to 0.34372, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.3755 - categorical_accuracy: 0.9841 - val_loss: 0.3437 - val_categorical_accuracy: 0.9814\n",
            "Epoch 6/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.2918 - categorical_accuracy: 0.9867\n",
            "Epoch 6: val_loss improved from 0.34372 to 0.27479, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.2917 - categorical_accuracy: 0.9867 - val_loss: 0.2748 - val_categorical_accuracy: 0.9829\n",
            "Epoch 7/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.2310 - categorical_accuracy: 0.9898\n",
            "Epoch 7: val_loss improved from 0.27479 to 0.21602, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.2310 - categorical_accuracy: 0.9898 - val_loss: 0.2160 - val_categorical_accuracy: 0.9895\n",
            "Epoch 8/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1942 - categorical_accuracy: 0.9899\n",
            "Epoch 8: val_loss improved from 0.21602 to 0.20533, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.1942 - categorical_accuracy: 0.9899 - val_loss: 0.2053 - val_categorical_accuracy: 0.9837\n",
            "Epoch 9/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1650 - categorical_accuracy: 0.9911\n",
            "Epoch 9: val_loss improved from 0.20533 to 0.17087, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.1650 - categorical_accuracy: 0.9911 - val_loss: 0.1709 - val_categorical_accuracy: 0.9865\n",
            "Epoch 10/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.1421 - categorical_accuracy: 0.9922\n",
            "Epoch 10: val_loss did not improve from 0.17087\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.1421 - categorical_accuracy: 0.9922 - val_loss: 0.1729 - val_categorical_accuracy: 0.9825\n",
            "Epoch 11/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1240 - categorical_accuracy: 0.9935\n",
            "Epoch 11: val_loss improved from 0.17087 to 0.13775, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.1240 - categorical_accuracy: 0.9935 - val_loss: 0.1378 - val_categorical_accuracy: 0.9868\n",
            "Epoch 12/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1124 - categorical_accuracy: 0.9935\n",
            "Epoch 12: val_loss improved from 0.13775 to 0.13481, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.1124 - categorical_accuracy: 0.9935 - val_loss: 0.1348 - val_categorical_accuracy: 0.9845\n",
            "Epoch 13/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.1011 - categorical_accuracy: 0.9941\n",
            "Epoch 13: val_loss improved from 0.13481 to 0.11902, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.1011 - categorical_accuracy: 0.9941 - val_loss: 0.1190 - val_categorical_accuracy: 0.9875\n",
            "Epoch 14/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0914 - categorical_accuracy: 0.9946\n",
            "Epoch 14: val_loss did not improve from 0.11902\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0914 - categorical_accuracy: 0.9947 - val_loss: 0.1198 - val_categorical_accuracy: 0.9865\n",
            "Epoch 15/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0850 - categorical_accuracy: 0.9948\n",
            "Epoch 15: val_loss improved from 0.11902 to 0.11062, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0850 - categorical_accuracy: 0.9948 - val_loss: 0.1106 - val_categorical_accuracy: 0.9868\n",
            "Epoch 16/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0770 - categorical_accuracy: 0.9957\n",
            "Epoch 16: val_loss did not improve from 0.11062\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0770 - categorical_accuracy: 0.9957 - val_loss: 0.1130 - val_categorical_accuracy: 0.9864\n",
            "Epoch 17/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0740 - categorical_accuracy: 0.9952\n",
            "Epoch 17: val_loss improved from 0.11062 to 0.09084, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0740 - categorical_accuracy: 0.9952 - val_loss: 0.0908 - val_categorical_accuracy: 0.9895\n",
            "Epoch 18/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0678 - categorical_accuracy: 0.9961\n",
            "Epoch 18: val_loss improved from 0.09084 to 0.08950, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0678 - categorical_accuracy: 0.9961 - val_loss: 0.0895 - val_categorical_accuracy: 0.9898\n",
            "Epoch 19/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0654 - categorical_accuracy: 0.9956\n",
            "Epoch 19: val_loss did not improve from 0.08950\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0654 - categorical_accuracy: 0.9956 - val_loss: 0.1066 - val_categorical_accuracy: 0.9831\n",
            "Epoch 20/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0622 - categorical_accuracy: 0.9960\n",
            "Epoch 20: val_loss improved from 0.08950 to 0.08488, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0622 - categorical_accuracy: 0.9960 - val_loss: 0.0849 - val_categorical_accuracy: 0.9877\n",
            "Epoch 21/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0597 - categorical_accuracy: 0.9962\n",
            "Epoch 21: val_loss improved from 0.08488 to 0.07792, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0597 - categorical_accuracy: 0.9962 - val_loss: 0.0779 - val_categorical_accuracy: 0.9898\n",
            "Epoch 22/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0562 - categorical_accuracy: 0.9966\n",
            "Epoch 22: val_loss did not improve from 0.07792\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0562 - categorical_accuracy: 0.9966 - val_loss: 0.0897 - val_categorical_accuracy: 0.9879\n",
            "Epoch 23/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0542 - categorical_accuracy: 0.9963\n",
            "Epoch 23: val_loss improved from 0.07792 to 0.07683, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0542 - categorical_accuracy: 0.9963 - val_loss: 0.0768 - val_categorical_accuracy: 0.9889\n",
            "Epoch 24/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0534 - categorical_accuracy: 0.9959\n",
            "Epoch 24: val_loss did not improve from 0.07683\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0534 - categorical_accuracy: 0.9959 - val_loss: 0.0844 - val_categorical_accuracy: 0.9868\n",
            "Epoch 25/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0502 - categorical_accuracy: 0.9967\n",
            "Epoch 25: val_loss improved from 0.07683 to 0.07377, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 56s 28ms/step - loss: 0.0502 - categorical_accuracy: 0.9967 - val_loss: 0.0738 - val_categorical_accuracy: 0.9900\n",
            "Epoch 26/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0486 - categorical_accuracy: 0.9966\n",
            "Epoch 26: val_loss did not improve from 0.07377\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0486 - categorical_accuracy: 0.9966 - val_loss: 0.0928 - val_categorical_accuracy: 0.9850\n",
            "Epoch 27/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0475 - categorical_accuracy: 0.9968\n",
            "Epoch 27: val_loss did not improve from 0.07377\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0475 - categorical_accuracy: 0.9968 - val_loss: 0.0864 - val_categorical_accuracy: 0.9856\n",
            "Epoch 28/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0459 - categorical_accuracy: 0.9968\n",
            "Epoch 28: val_loss did not improve from 0.07377\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0459 - categorical_accuracy: 0.9969 - val_loss: 0.0740 - val_categorical_accuracy: 0.9891\n",
            "Epoch 29/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0450 - categorical_accuracy: 0.9968\n",
            "Epoch 29: val_loss did not improve from 0.07377\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0450 - categorical_accuracy: 0.9968 - val_loss: 0.0754 - val_categorical_accuracy: 0.9889\n",
            "Epoch 30/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0437 - categorical_accuracy: 0.9969\n",
            "Epoch 30: val_loss did not improve from 0.07377\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0437 - categorical_accuracy: 0.9969 - val_loss: 0.0774 - val_categorical_accuracy: 0.9880\n",
            "Epoch 31/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0409 - categorical_accuracy: 0.9975\n",
            "Epoch 31: val_loss did not improve from 0.07377\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0409 - categorical_accuracy: 0.9975 - val_loss: 0.0774 - val_categorical_accuracy: 0.9872\n",
            "Epoch 32/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0420 - categorical_accuracy: 0.9969\n",
            "Epoch 32: val_loss improved from 0.07377 to 0.07230, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0420 - categorical_accuracy: 0.9969 - val_loss: 0.0723 - val_categorical_accuracy: 0.9890\n",
            "Epoch 33/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0418 - categorical_accuracy: 0.9968\n",
            "Epoch 33: val_loss did not improve from 0.07230\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0418 - categorical_accuracy: 0.9969 - val_loss: 0.0906 - val_categorical_accuracy: 0.9818\n",
            "Epoch 34/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0392 - categorical_accuracy: 0.9974\n",
            "Epoch 34: val_loss did not improve from 0.07230\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0392 - categorical_accuracy: 0.9974 - val_loss: 0.0767 - val_categorical_accuracy: 0.9859\n",
            "Epoch 35/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0400 - categorical_accuracy: 0.9967\n",
            "Epoch 35: val_loss improved from 0.07230 to 0.07124, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0400 - categorical_accuracy: 0.9967 - val_loss: 0.0712 - val_categorical_accuracy: 0.9878\n",
            "Epoch 36/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0388 - categorical_accuracy: 0.9972\n",
            "Epoch 36: val_loss improved from 0.07124 to 0.06017, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0388 - categorical_accuracy: 0.9972 - val_loss: 0.0602 - val_categorical_accuracy: 0.9922\n",
            "Epoch 37/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0359 - categorical_accuracy: 0.9978\n",
            "Epoch 37: val_loss did not improve from 0.06017\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0359 - categorical_accuracy: 0.9978 - val_loss: 0.0615 - val_categorical_accuracy: 0.9910\n",
            "Epoch 38/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0372 - categorical_accuracy: 0.9972\n",
            "Epoch 38: val_loss did not improve from 0.06017\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0372 - categorical_accuracy: 0.9972 - val_loss: 0.0692 - val_categorical_accuracy: 0.9881\n",
            "Epoch 39/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0361 - categorical_accuracy: 0.9973\n",
            "Epoch 39: val_loss did not improve from 0.06017\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0361 - categorical_accuracy: 0.9973 - val_loss: 0.0637 - val_categorical_accuracy: 0.9899\n",
            "Epoch 40/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0363 - categorical_accuracy: 0.9974\n",
            "Epoch 40: val_loss did not improve from 0.06017\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0363 - categorical_accuracy: 0.9974 - val_loss: 0.0602 - val_categorical_accuracy: 0.9906\n",
            "Epoch 41/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0344 - categorical_accuracy: 0.9978\n",
            "Epoch 41: val_loss did not improve from 0.06017\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0344 - categorical_accuracy: 0.9978 - val_loss: 0.0664 - val_categorical_accuracy: 0.9882\n",
            "Epoch 42/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0356 - categorical_accuracy: 0.9973\n",
            "Epoch 42: val_loss did not improve from 0.06017\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0356 - categorical_accuracy: 0.9973 - val_loss: 0.0712 - val_categorical_accuracy: 0.9877\n",
            "Epoch 43/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0343 - categorical_accuracy: 0.9974\n",
            "Epoch 43: val_loss improved from 0.06017 to 0.05987, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0343 - categorical_accuracy: 0.9974 - val_loss: 0.0599 - val_categorical_accuracy: 0.9900\n",
            "Epoch 44/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0332 - categorical_accuracy: 0.9976\n",
            "Epoch 44: val_loss did not improve from 0.05987\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0332 - categorical_accuracy: 0.9976 - val_loss: 0.0602 - val_categorical_accuracy: 0.9898\n",
            "Epoch 45/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0325 - categorical_accuracy: 0.9978\n",
            "Epoch 45: val_loss did not improve from 0.05987\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0326 - categorical_accuracy: 0.9977 - val_loss: 0.0788 - val_categorical_accuracy: 0.9863\n",
            "Epoch 46/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0343 - categorical_accuracy: 0.9969\n",
            "Epoch 46: val_loss did not improve from 0.05987\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0343 - categorical_accuracy: 0.9969 - val_loss: 0.0611 - val_categorical_accuracy: 0.9902\n",
            "Epoch 47/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0320 - categorical_accuracy: 0.9979\n",
            "Epoch 47: val_loss did not improve from 0.05987\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0319 - categorical_accuracy: 0.9979 - val_loss: 0.0676 - val_categorical_accuracy: 0.9869\n",
            "Epoch 48/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0302 - categorical_accuracy: 0.9979\n",
            "Epoch 48: val_loss did not improve from 0.05987\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0302 - categorical_accuracy: 0.9979 - val_loss: 0.0758 - val_categorical_accuracy: 0.9864\n",
            "Epoch 49/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0324 - categorical_accuracy: 0.9973\n",
            "Epoch 49: val_loss did not improve from 0.05987\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0324 - categorical_accuracy: 0.9973 - val_loss: 0.0679 - val_categorical_accuracy: 0.9890\n",
            "Epoch 50/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0301 - categorical_accuracy: 0.9981\n",
            "Epoch 50: val_loss did not improve from 0.05987\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0301 - categorical_accuracy: 0.9981 - val_loss: 0.0656 - val_categorical_accuracy: 0.9889\n",
            "Epoch 51/80\n",
            "2000/2000 [==============================] - ETA: 0s - loss: 0.0305 - categorical_accuracy: 0.9979\n",
            "Epoch 51: val_loss did not improve from 0.05987\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0305 - categorical_accuracy: 0.9979 - val_loss: 0.0693 - val_categorical_accuracy: 0.9881\n",
            "Epoch 52/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0308 - categorical_accuracy: 0.9975\n",
            "Epoch 52: val_loss improved from 0.05987 to 0.05945, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0308 - categorical_accuracy: 0.9976 - val_loss: 0.0594 - val_categorical_accuracy: 0.9903\n",
            "Epoch 53/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0280 - categorical_accuracy: 0.9984\n",
            "Epoch 53: val_loss did not improve from 0.05945\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0280 - categorical_accuracy: 0.9984 - val_loss: 0.0827 - val_categorical_accuracy: 0.9851\n",
            "Epoch 54/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0307 - categorical_accuracy: 0.9973\n",
            "Epoch 54: val_loss improved from 0.05945 to 0.05291, saving model to ./2_lyr_srn_1conv\n",
            "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0307 - categorical_accuracy: 0.9973 - val_loss: 0.0529 - val_categorical_accuracy: 0.9908\n",
            "Epoch 55/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0298 - categorical_accuracy: 0.9978\n",
            "Epoch 55: val_loss did not improve from 0.05291\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0298 - categorical_accuracy: 0.9978 - val_loss: 0.0649 - val_categorical_accuracy: 0.9891\n",
            "Epoch 56/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0298 - categorical_accuracy: 0.9979\n",
            "Epoch 56: val_loss did not improve from 0.05291\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0298 - categorical_accuracy: 0.9979 - val_loss: 0.0567 - val_categorical_accuracy: 0.9904\n",
            "Epoch 57/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0277 - categorical_accuracy: 0.9981\n",
            "Epoch 57: val_loss did not improve from 0.05291\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0277 - categorical_accuracy: 0.9981 - val_loss: 0.0667 - val_categorical_accuracy: 0.9878\n",
            "Epoch 58/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0289 - categorical_accuracy: 0.9977\n",
            "Epoch 58: val_loss did not improve from 0.05291\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0289 - categorical_accuracy: 0.9977 - val_loss: 0.0548 - val_categorical_accuracy: 0.9900\n",
            "Epoch 59/80\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 0.0300 - categorical_accuracy: 0.9974\n",
            "Epoch 59: val_loss did not improve from 0.05291\n",
            "2000/2000 [==============================] - 55s 27ms/step - loss: 0.0300 - categorical_accuracy: 0.9974 - val_loss: 0.0607 - val_categorical_accuracy: 0.9890\n",
            "Epoch 60/80\n",
            " 329/2000 [===>..........................] - ETA: 42s - loss: 0.0275 - categorical_accuracy: 0.9982"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m5,_ \u001b[38;5;241m=\u001b[39m AffineModel()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm5\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2_lyr_srn_1conv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, path, epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m checkpoint1 \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpath,save_format\u001b[38;5;241m=\u001b[39mtf,monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                           save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m                            verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      5\u001b[0m                            save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                            mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[1;32m      8\u001b[0m   loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m   metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:1748\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1746\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1747\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1748\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:694\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/nest.py:624\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1054\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1054\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:687\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 687\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1141\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1107\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1106\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "m5,_ = AffineModel()\n",
        "train(m5,'2_lyr_srn_1conv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-15T00:13:32.458513Z",
          "iopub.status.busy": "2023-12-15T00:13:32.458145Z",
          "iopub.status.idle": "2023-12-15T00:13:32.465031Z",
          "shell.execute_reply": "2023-12-15T00:13:32.464026Z",
          "shell.execute_reply.started": "2023-12-15T00:13:32.458485Z"
        },
        "id": "bC5pubc5O6ec"
      },
      "outputs": [],
      "source": [
        "def train(model,path,epochs=80):\n",
        "  checkpoint1 = ModelCheckpoint(filepath='./'+path,save_format=tf,monitor='val_loss',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics='sparse_categorical_accuracy')\n",
        "  model.fit(labeled_train_dataset, epochs=epochs, batch_size=30, validation_data=labeled_val_dataset, callbacks=[checkpoint1,reduce_lr], verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-04T13:02:35.143645Z",
          "iopub.status.busy": "2024-02-04T13:02:35.142668Z",
          "iopub.status.idle": "2024-02-04T13:02:35.148002Z",
          "shell.execute_reply": "2024-02-04T13:02:35.146960Z",
          "shell.execute_reply.started": "2024-02-04T13:02:35.143599Z"
        },
        "id": "nwdCY44ZO6ec"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-03T22:38:47.066697Z",
          "iopub.status.busy": "2023-12-03T22:38:47.066227Z",
          "iopub.status.idle": "2023-12-03T22:38:47.074068Z",
          "shell.execute_reply": "2023-12-03T22:38:47.072887Z",
          "shell.execute_reply.started": "2023-12-03T22:38:47.066659Z"
        },
        "id": "kC3mJkhWO6ec"
      },
      "outputs": [],
      "source": [
        "def train(model,path,epochs=80):\n",
        "  checkpoint1 = ModelCheckpoint(filepath='./'+path,save_format=tf,monitor='val_loss',\n",
        "                            save_weights_only=True,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics='sparse_categorical_accuracy')\n",
        "  model.fit(labeled_train_dataset, epochs=epochs, batch_size=200, validation_data=labeled_val_dataset, callbacks=[checkpoint1], verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T21:13:22.399956Z",
          "iopub.status.busy": "2023-11-20T21:13:22.399159Z",
          "iopub.status.idle": "2023-11-20T21:13:22.405128Z",
          "shell.execute_reply": "2023-11-20T21:13:22.40402Z",
          "shell.execute_reply.started": "2023-11-20T21:13:22.399904Z"
        },
        "id": "rLlpA9ZKO6ec"
      },
      "outputs": [],
      "source": [
        "def edim(inp,axis=-1):\n",
        "    return tf.expand_dims(inp,axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8rVnQi6O6ed"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def test_invariance(model):\n",
        "  print(model.evaluate(testXA,testY))\n",
        "  affine_rot(model,5)\n",
        "  affine_rot(model,10)\n",
        "  affine_rot(model,20)\n",
        "  affine_rot(model,30)\n",
        "  affine_rot(model,50)\n",
        "  affine_rot(model,70)\n",
        "  affine_scale(model,1.1)\n",
        "  affine_scale(model,1.2)\n",
        "  affine_scale(model,1.3)\n",
        "  affine_scale(model,1.4)\n",
        "  affine_scale(model,2.0)\n",
        "  affine_shear(model,0.1)\n",
        "  affine_shear(model,0.2)\n",
        "  affine_shear(model,0.4)\n",
        "  affine_shear(model,0.5)\n",
        "\n",
        "def affine_rot(model,theta):\n",
        "    tr = tfa.image.rotate(testXA,(theta/180)*math.pi)\n",
        "    image2 = tr\n",
        "    #print(model.evaluate(testXA,testY))\n",
        "    #indices = tf.where(tf.argmax(model(test_images),-1)==tf.argmax(test_labels),-1)\n",
        "    #print(model.evaluate(image2[indices],test_labels[indices]))\n",
        "    print(model.evaluate(tr,testY))\n",
        "\n",
        "def affine_scale(model,scale):\n",
        "    tr =  tfa.image.transform(testXA,[scale,0.0,0.0,0.0,scale,0,0,0])\n",
        "    image2=tr\n",
        "    #print(model.evaluate(testXA,testY))\n",
        "    print(model.evaluate(image2,testY))\n",
        "\n",
        "def affine_shear(model,scale):\n",
        "    total_acc=0\n",
        "    n=0\n",
        "    #predictions1 = model.predict(testXA)\n",
        "    image2 = []\n",
        "    labels = []\n",
        "    #image2 =np.array(image2)\n",
        "    tr = tfa.image.transform(testXA,[1.0,scale,0,scale,1.0,0,0,0])#tfa.image.shear_x(testXA,scale,replace=0.0) #\n",
        "    image2=tr\n",
        "    #print(model.evaluate(testXA,testY))\n",
        "    print(model.evaluate(tr,testY))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-y2T4osO6ee"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}